\chapter{Continuations}
\labch{continuations}

\renewcommand{\Lang}{\textsf{FAE}\xspace}

Many real-world languages support \textit{control diverters}, which alter
\textit{control flows} of programs.\index{control diverter}\index{control flow}
For example, programmers can use \code{return} in Scala code. Consider the
following Scala program:

\begin{verbatim}
def foo(x: Int): Int = {
  val y = return x
  x + x
}

foo(3)
\end{verbatim}

Its result is \code{3}, not \code{6}. When \code{return x} is evaluated, the
value denoted by \code{x} is immediately returned by the function. \code{x + x}
is never evaluated. It shows how \code{return} is different from many other
expressions, including addition and function application. Most expressions
produce values as results. However, \code{return} changes
the control flow by making the function immediately return. We can find various
control diverters other than \code{return} in real-world languages:
\code{break}, \code{continue}, \code{goto}, \code{throw} (or \code{raise}), and
so on.

Control diverters are useful for writing programs with complex control flows.
For instance, consider a function \code{numOfWordsInFile} that takes the name of
a file and a string as arguments and returns how many times the string occurs in
the file.

\begin{verbatim}
def numOfWordsInFile(name: String, word: String): Int = ...
\end{verbatim}

If such a file does not exist, the function returns \code{-1}.
When the file is read for the first time, its content is cached, so the function
must check the cache first to reuse the cached result if available.

Assume that we have the following helper functions:

\begin{verbatim}
// checks whether a cached result exists
def cached(name: String): Boolean

// gets the cached result
def getCache(name: String): String

// checks whether a given file exists
def exists(name: String): Boolean

// reads the content of the file and caches it
def read(name: String): String

// counts the number of occurrences of `word` in `content`
def numOfWords(content: String, word: String): Int
\end{verbatim}

Then, we can implement \code{numOfWordsInFile} with the helper functions and
\code{return}.

\begin{verbatim}
def numOfWordsInFile(name: String, word: String): Int = {
  val content =
    if (cached(name))
      getCache(name)
    else if (exists(name))
      read(name)
    else
      return -1
  numOfWords(content, word)
}
\end{verbatim}

First, the function checks whether there is a cached result by calling \code{cached}.
If so, the cached result is acquired with \code{getCache} and stored in the variable
\code{content}. Otherwise, the function checks whether the file exists with
\code{exists}. The file is read with \code{read} if the file exists, and its
content is stored in \code{content}. When the file is missing, the function
immediately returns \code{-1} with \code{return -1}. Finally, the function
counts the number of \code{word} in \code{content} with \code{numOfWords} to
compute the result. Note that when both cached result and file are absent,
\code{numOfWords} is never called because \code{return -1} terminates the
function beforehand.

Without \code{return}, we need to call \code{numOfWords} in multiple places.

\begin{verbatim}
def numOfWordsInFile(name: String, word: String): Int = {
  if (cached(name))
    numOfWords(getCache(name), word)
  else if (exists(name))
    numOfWords(read(name), word)
  else
    -1
}
\end{verbatim}

The code looks fine, but we cannot directly express the idea that the function needs to
call \code{numOfWords} except one errorneous case, where both cache and file are
not found. It is not a big flaw in the current implementation of
\code{numOfWordsInFile}.
However, if we write a function with a large number of conditions, we would
prefer \code{return} (\reffig{return}) to call \code{numOfWords} multiple
times (\reffig{multiple}).

\begin{figure}[t]
\begin{verbatim}
def numOfWordsInFile(name: String, word: String): Int = {
  val content =
    if (A)
      a
    else if (B)
      b

    ...

    else if (F)
      f
    else
      return -1
  numOfWords(content, word)
}
\end{verbatim}
\caption{\code{numOfWordsInFile} with \code{return}}
\labfig{return}
\end{figure}

\begin{figure}[t]
\begin{verbatim}
def numOfWordsInFile(name: String, word: String): Int = {
  if (A)
    numOfWords(a, word)
  else if (B)
    numOfWords(b, word)

  ...

  else if (F)
    numOfWords(f, word)
  else
    -1
}
\end{verbatim}
\caption{\code{numOfWordsInFile} without \code{return}}
\labfig{multiple}
\end{figure}

Like mutation, control diverters make languages impure. In pure languages, the
order of evaluation does not matter. Each expression only produces a result;
there is no other \textit{side effect}.\index{side effect}
On the other hand, in impure languages, the order
of evaluation matters. Expressions can perform side effects, including mutation
and control flow changes. Evaluating a certain expression can change the result
of other expressions or make other expressions not evaluated. Therefore,
programs written in impure languages require global reasoning, while programs
written in pure languages require local, modular reasoning.
Mutation and control diverters make the reasoning of programs difficult despite
their usefulness. Control diverters must be used with extra care of programmers.

This chapter and the following two chapters introduce continuations, which are
the most general explanation of control flow. This chapter focuses on what
continuations are and how we can write programs and define semantics while
exposing continuations explicitly. Then, the next chapter explains how we can
add control diverters to languages based on the notion of a continuation.

\section{Redexes and Continuations}

Evaluating an expression requires one or more steps of computation.
Consider $(1+2)+(3+4)$. Evaluation of this expression consists of the following
seven steps of computation:\footnote{Assume that we use the left-to-right order
for subexpression evaluation in this chapter.}

\begin{enumerate}
  \item Evaluate the expression $1$ to get the integer value $1$.
  \item Evaluate the expression $2$ to get the integer value $2$.
  \item Add the integer value $1$ to the integer value $2$ to get the integer
    value $3$.
  \item Evaluate the expression $3$ to get the integer value $3$.
  \item Evaluate the expression $4$ to get the integer value $4$.
  \item Add the integer value $3$ to the integer value $4$ to get the integer
    value $7$.
  \item Add the integer value $3$ to the integer value $7$ to get the integer
    value $10$.
\end{enumerate}

We can split $N$ steps of computation into two parts: the former $n$ steps and the
remaining $N-n$ steps. We call the expression evaluated by the former $n$ steps
a \textit{redex}\index{redex}\footnote{The term redex stands for a
\textbf{red}ucible \textbf{ex}pression. However, we introduce the notion of a
redex without explaining what ``reduction'' or ``reducible'' is. The notion can
be understood without knowing what reduction is, so we do not care about the
origin of the term.} and the remaining computation described by the
latter $N-n$ steps the \textit{continuation}\index{continuation} of the redex.

For example, if we split the above steps into step 1 and steps 2-7, then the redex is
the expression $1$, and the continuation consists of the remaining steps.
The important point is that the continuation requires the result of the redex to
complete the evaluation. Without $1$, the result of step 1, the continuation
cannot proceed beyond step 3. Step 3 can be accomplished only when the result of
the redex is provided. Therefore, we can consider the continuation as an
expression with a hole that must be filled with the result of a redex.
Intuitively, the continuation of $1$ can be written as $(\square+2)+(3+4)$, where
$\square$ denotes the place in which the result of the redex is used.
Since the continuation takes the result of a redex as input and completes the
remaining computation, the continuation can be interpreted as a function.
Following this interpretation, we can express the continuation of $1$ as
$\efun{\cx}{(\cx+2)+(3+4)}$.

There are multiple ways to split the steps. The following table shows
three different ways of splitting the evaluation of $(1+2)+(3+4)$ to find a redex and the
continuation.

\begin{tabular}{|c|@{~}c|c|@{~}c@{~}|@{~}c@{~}|}
  \hline
  \multicolumn{2}{|c|}{Redex}&\multicolumn{3}{c|}{Continuation} \\
  \hline
  Steps & Expression & Steps & Hole & Function \\
  \hline
  \hline
  1 & $1$ & 2-7 & $(\square+2)+(3+4)$ & $\efun{\cx}{(\cx+2)+(3+4)}$ \\
  \hline
  1-3 & $1+2$ & 4-7 & $\square+(3+4)$ & $\efun{\cx}{\cx+(3+4)}$ \\
  \hline
  1-7 & $(1+2)+(3+4)$ & $\cdot$ & $\square$ & $\efun{\cx}{\cx}$ \\
  \hline
\end{tabular}

Note that $2$, $3$, $4$, and $3+4$ are not redexes, while $1$, $1+2$,
and $(1+2)+(3+4)$ are redexes. A redex is an expression that can be evaluated first.
Since $2$ cannot be evaluated until $1$ is evaluated, $2$ is not a redex.
Similarly, $3$, $4$, and $3+4$ cannot be evaluated until $1+2$ is evaluated, so
they are not redexes. On the other hand, $1+2$ is a redex because there is
nothing need to be done before the evaluation of $1+2$.

Since a continuation also consists of multiple steps of computation, it can
split again into a redex and the continuation of the redex. For example,
consider the continuation of $1$, which consists of steps 2-7.
If we split it into step 2 and steps 3-7, the redex is the expression
$2$, and the continuation is $(\underline{1}+\square)+(3+4)$. Here, the
line below $1$ expresses that $1$ is an integer value, not an expression.

Therefore, evaluation of an expression repeats evaluation of a redex and
application of a continuation. A given expression splits into a redex and a
continuation. The redex evaluates to a value, and the continuation is applied
to the value. Then, the continuation splits again into a redex and a
continuation, and the redex is evaluated. This process repeats until there is no
more remaining computation, i.e. the continuation becomes the identity function.

\section{Continuation-Passing Style}

\textit{Continuation-passing style}\index{continuation-passing style}
(\acrshort{cpsLabel}) is a style of programming that
passes remaining computations to function calls. In CPS, programs never
use return values; they pass continuations as arguments instead. Therefore, by
writing programs in CPS, continuations become explicit in the source code of the
programs.

Before moving on to the detail of CPS, recalling
store-passing style, used in \refch{boxes} and \refch{mutable-variables}, would help you
understand CPS. Those chapters
choose to use store passing in order to show how we can implement mutation
without mutation. The following code is part of the implementation of a
\textsf{BFAE} interpreter in \refch{boxes}.
\begin{verbatim}
def interp(e: Expr, env: Env, sto: Sto): (Value, Sto) =
  e match {
    ...
    case Add(l, r) =>
      val (NumV(n), ls) = interp(l, env, sto)
      val (NumV(m), rs) = interp(r, env, ls)
      (NumV(n + m), rs)
    case NewBox(e) =>
      val (v, s) = interp(e, env, sto)
      val a = s.keys.maxOption.getOrElse(0) + 1
      (BoxV(a), s + (a -> v))
  }
\end{verbatim}
A store-passing interpreter passes the current store to each function call, and
each function call returns its resulting store.

If we use mutable maps, we can implement interpreters of
\textsf{BFAE} and \textsf{MFAE} without store-passing style. The following code
is part of the implementation of a \textsf{BFAE} interpreter using a mutable map.
\begin{verbatim}
type Sto = scala.collection.mutable.Map[Addr, Value]
val sto: Sto = scala.collection.mutable.Map()

def interp(e: Expr, env: Env): Value =
  e match {
    ...
    case Add(l, r) =>
      val NumV(n) = interp(l, env)
      val NumV(m) = interp(r, env)
      NumV(n + m)
    case NewBox(e) =>
      val v = interp(e, env)
      val a = sto.keys.maxOption.getOrElse(0) + 1
      sto += (a -> v)
      BoxV(a)
  }
\end{verbatim}
The variable \code{sto} denotes a mutable map. The function \code{interp}
depends on \code{sto}, instead of passing stores as an argument and a return value.
The \code{Add} case does not pass the resulting store from the evaluation of
\code{l} to the evaluation of \code{r}.
The \code{NewBox} case simply mutates \code{sto} to create a new box.
Note that \code{sto += (a -> v)} mutates the map by adding a mapping from
\code{a} to \code{v}.
Store passing is unnecessary since there is a global, mutable map, which records every update.

Two code snippets clearly compare interpreters with and without store passing.
In the former, with store passing, the current store at each point of execution
is explicit. When we see the \code{Add} case, it is clear that
\code{sto} is used for the evaluation of \code{l}, which may change the store,
and the resulting store of \code{l} is used for the evaluation of \code{r}.
However, in the latter, without store passing, the current store at each point
is implicit. The code does not reveal the fact that \code{interp(l,
env)} can change the store and, therefore, affect the result of \code{interp(r,
env)}. Implementation with store-passing style explicitly shows the use and flow of
stores by passing stores from functions to functions, while implementation
without store-passing style hides the use and flow of stores and makes the code
shorter.

CPS is similar to store-passing style. The difference is that CPS passes
continuations, while store-passing style passes stores. Like that store-passing
style exposes the store used by each function application, CPS exposes the
continuation of each function application.

This section illustrates how we can write programs in CPS by giving factorial as
an example. Consider a function calculating the factorial of a given integer.
The following function does not use CPS:
\begin{verbatim}
def factorial(n: Int): Int =
  if (n <= 1)
    1
  else
    n * factorial(n - 1)
\end{verbatim}
Since \code{factorial} does not use CPS, the continuation is implicit.
For example, in \code{factorial(5) + 1},
the continuation of \code{factorial(5)} is to add \code{1} to the
result, i.e. \code{x => x + 1}. Although the continuation of \code{factorial(5)}
does exist and is executed during the evaluation of \code{factorial(5) + 1},
we cannot find \code{x => x + 1} in the code per se. The reason is that
\code{factorial} does not use CPS.

Let us transform this function to use CPS. Since each function in CPS takes a
continuation as an argument, the first thing to do is to add a parameter to a
function. The continuation of a function application uses the return value of
certain computation.
Therefore, a continuation can be interpreted as a function that takes
the return value as input. In the case of \code{factorial}, the continuation
takes an integer as input.
On the other hand, there is no restriction on what the continuation
computes; it can do whatever it wants. In \code{factorial(5) + 1}, the
continuation of \code{factorial(5)} results in an integer. At the same time,
\code{factorial(5) + 1} results in an integer, too. In \code{120 == factorial(5)},
the continuation of \code{factorial(5)}, which is \code{x => 120 == x}, results in a boolean. The whole
expression \code{120 == factorial(5)} also results in a boolean.
Therefore, the output of a continuation can have any type,
but the type must be the same as the type of the whole expression.

Based on these observations, we can define the type of the
continuation of \code{factorial}.
It is a function type whose parameter type is \code{Int}.
The return type can be any type, but
for brevity, we fix the return type to \code{Int}.

\begin{verbatim}
type Cont = Int => Int
\end{verbatim}

Now, we can add a parameter that denotes the
continuation of a function call to \code{factorial}.
We call this new function \code{factorialCps}.

\begin{verbatim}
def factorialCps(n: Int, k: Cont): Int = ...
\end{verbatim}

\code{k} is the continuation of the function. Thus,
\code{factorialCps(n, k)} means evaluating \code{factorial(n)} where its
continuation is \code{k}. According to the definition of a continuation,
\code{k(factorial(n))} must equal \code{factorialCps(n, k)}.
The return type of \code{factorialCps} must be the same as the return type of
\code{k}. Since the return type of \code{k} is fixed to \code{Int}, the return
type of \code{factorialCps} also is \code{Int}.

% The above function is not valid in Scala because it uses an unknown type
% \code{?}. The idea we want to express is that the return type of \code{k} can be
% any type but it must be the same as the return type of \code{factorialCps}.
% We can encode the idea with parametric polymorphism of Scala. Since parametric
% polymorphism is the topic of \refch{parametric-polymorphism}, we do not explain
% the detail here.

% The type of \code{k} is \code{Int => T}. It denotes that the second
% argument for \code{factorialCps} can has any return type. Since the return type
% of \code{factorialCps} also is \code{T}, the return type of \code{k} must be the
% same as \code{factorialCps}. For example, if the second argument for
% \code{factorialCps} has the type \code{Int => Int}, \code{factorialCps} must return an
% integer. If the second argument has the type \code{Int => Boolean},
% \code{factorial} must return a boolean.

The most naïve implementation of \code{factorialCps} is as follows:

\begin{verbatim}
def factorialCps(n: Int, k: Cont): Int =
  k(factorial(n))
\end{verbatim}

Obviously, it is not the correct implementation of \code{factorialCps} because
it still depends on \code{factorial}; we want \code{factorialCps} to be
independent of \code{factorial}.
The current version is just a specification, not an implementation, of \code{factorialCps}.
However, it is a good starting point. We can replace \code{factorial(n)} with
the body of \code{factorial} to obtain the following code:

\begin{verbatim}
def factorialCps(n: Int, k: Cont): Int =
  k(
    if (n <= 1)
      1
    else
      n * factorial(n - 1)
  )
\end{verbatim}

Note that \code{f(if (e1) e2 else e3)} is the same as \code{if (e1) f(e2) else
f(e3)}. Therefore, the above code is equivalent to the following code:

\begin{verbatim}
def factorialCps(n: Int, k: Cont): Int =
  if (n <= 1)
    k(1)
  else
    k(n * factorial(n - 1))
\end{verbatim}

It looks better than before but still depends on \code{factorial}, which is not
a function written in CPS. The use of \code{factorial} must disappear. Now, the
goal is eliminating \code{factorial} in \code{k(n * factorial(n - 1))}. It is
possible by using \code{factorialCps} instead of \code{factorial}. The key
intuition to achieve the goal is to recall that \code{k(factorial(n))} equals
\code{factorialCps(n, k)}. Based on the equality, we can conclude that the following
equation is true:

$
\begin{array}{cl}
& \code{k(n * factorial(n - 1))} \\
= & \code{(x => k(n * x))(factorial(n - 1))} \\
= & \code{factorialCps(n - 1, x => k(n * x))} \\
\end{array}
$

\code{(x => k(n * x))(factorial(n - 1))} applies \code{x => k(n * x)} to the
result of \code{factorial(n - 1)} and, thus, equals \code{k(n * factorial(n -
1))}. Since \code{k(factorial(n))} equals \code{factorialCps(n, k)},
\code{(x => k(n * x))(factorial(n - 1))} equals
\code{factorialCps(n - 1, x => k(n * x))}. By utilizing the equality, we attain
the following code:

\begin{verbatim}
def factorialCps(n: Int, k: Cont): Int =
  if (n <= 1)
    k(1)
  else
    factorialCps(n - 1, x => k(n * x))
\end{verbatim}

The function uses CPS because its recursive call explicitly passes the
continuation as an argument. When \code{n} is greater than one,
\code{factorialCps(n, k)} computes $(\code{n}-1)!$, multiplies the result by
\code{n}, and applies \code{k} to the result of the multiplication. The first
step, computing $(\code{n}-1)!$, is done by calling \code{factorialCps} itself.
The subsequent two steps are the continuation of the recursive call. In the
implementation, the continuation is \code{x => k(n * x)}. It exactly coincides
with the aforementioned steps: multiplying the result by \code{n} and applying
\code{k}.

Now, we can compute $5!$ with \code{factorialCps} by writing
\code{factorialCps(5, x => x)}.
The continuation is \code{x => x} because there is nothing more to do with
$5!$, which is the desired result. In \code{factorial(5)}, the continuation is
implicit since \code{x => x} is not written in the code. On the other hand,
\code{x => x} is explicitly written in \code{factorialCps(5, x => x)}, which
clearly illustrates the main characteristic of CPS.
Similarly, to compute $5!+1$, we can write \code{factorialCps(5, x => x + 1)}
instead of \code{factorial(5) + 1}. To obtain $5!+1$ from $5!$, the only thing
to do is adding $1$. Therefore, the continuation is \code{x => x + 1}. Just like
before, the code with \code{factorialCps} directly shows the continuation,
while the code with \code{factorial} does not.

Since the output type of a continuation is \code{T}, any code using
\code{factorial} can be rewritten with \code{factorialCps}. For example,
\code{factorial(5) \% 2 == 0} checks whether $5!$ is an even integer. It is
equivalent to \code{factorialCps(5, x => x \% 2 == 0)}, which explicitly shows
the continuation. Similarly, \code{println(factorial(5))} prints \code{120},
which is $5!$. It is the same as \code{factorialCps(5, println)}, which also
reveals the continuation.

The code written in CPS has the following characteristics:
\begin{itemize}
  \item Each function takes a continuation as an argument, and each function
    application passes a continuation as an argument.
  \item A continuation is used---called or passed to another function---
    once and at most once in a function body.
  \item The return value of every function application is never used.
  \item Every function call is a tail call.
  \item Every function ends with a function call.
\end{itemize}
These are not individual ones; they are connected and express the same idea.
Since a continuation is given as an argument, the only way to finish a
computation is calling the continuation. Therefore, a continuation is used
once and at most once in a function body. Also, there is no need to do additional
computation with the return value of a function application.
The continuation does every additional
computation with the return value, so return values are not used at all.
Since we do not use return
values, every call is a tail call. Once a function calls another
function, the result of the callee is the result of caller. Moreover, there is no
way of returning from a function without calling any function. If the function
is the last step of a computation, it must call its continuation. Otherwise,
it needs to call another function to proceed the computation. Therefore, every
function ends with a function call.

% Writing functions with an arbitrary type \code{T} helps us check whether CPS is
% correctly used. In the body of \code{def factorialCps[T](n: Int, k: Int => T):
% T}, the only way of obtaining the value of \code{T} is using \code{k}. Since
% \code{T} is an arbitrary type, a value of the type \code{T} cannot be
% constructed without \code{k}. For example, the following function, which does
% not use CPS correclty, is rejected by type checking:

% \begin{verbatim}
% def factorialCps[T](n: Int, k: Int => T): T =
%   if (n <= 1)
%     k(1)
%   else
%     factorialCps(n - 1, x => n * x)
% \end{verbatim}

% The last line does not apply the continuation by mistake.
% For this reason, the type of the last line is \code{Int}, not \code{T}, and the
% mistake can be detected by the compiler.
% The use of \code{T} as the return type of \code{factorialCps}
% enforces the function to use \code{k}.

% While using \code{T} helps us to check the correctness, \code{T} can be replaced
% with any concrete type without breaking the code. For example, if
% \code{factorialCps} uses CPS correctly, we can change its signature to
% \code{def factorialCps(n: Int, k: Int => Int): Int} without changing the
% behavior of \code{factorialCps}. Note that the change may another part of the
% program that uses \code{factorialCps} becomes problematic despite its harmless
% to \code{factorialCps} itself. If there is an expression \code{factorialCps(5,
% println)}, it will not work after the change because \code{println} returns
% \code{Unit}, not \code{Int}. Using a concrete type instead of \code{T} in
% \code{factorialCps} makes the function less general but never makes the function
% wrong.

While CPS may seem to be needlessly complex, it is useful in various cases.
If we compare \code{factorial} and \code{factorialCps}, the former looks more concise.
It is difficult to implement programs correctly in CPS. One benefit of CPS is
that it makes every function call be a tail call. If implementation languages
support tail-call optimization, CPS can be used to avoid stack overflow.
However, this book uses Scala, which optimizes only tail-recursive calls.
Scala programs written in CPS can suffer from stack overflow despite the use of
CPS. Then, why does this section introduce CPS? The first reason is to help
readers understand the notion of a continuation. The other reason is that
the characteristic of CPS, passing a continuation explicitly as a value, is
sometimes useful. The next chapter shows such an example: an interpreter of a
language with first-class continuations. We will see how CPS can
contribute to the implementation of an interpreter in the next chapter.

\section{Interpreter in CPS}

Let us implement an interpreter of \Lang in CPS. As explained already, there is
no reason to implement an interperter of \Lang in CPS. However, CPS is an appropriate
implementation strategy for the interpreter of the next chapter, and this
section is preparation for the next chapter.

First, recall the previous implementation:

\begin{verbatim}
def interp(e: Expr, env: Env): Value = e match {
  case Num(n) => NumV(n)
  case Add(l, r) =>
    val v1 = interp(l, env)
    val v2 = interp(r, env)
    val NumV(n) = v1
    val NumV(m) = v2
    NumV(n + m)
  case Sub(l, r) =>
    val v1 = interp(l, env)
    val v2 = interp(r, env)
    val NumV(n) = v1
    val NumV(m) = v2
    NumV(n - m)
  case Id(x) => env(x)
  case Fun(x, b) => CloV(x, b, env)
  case App(f, a) =>
    val fv = interp(f, env)
    val av = interp(a, env)
    val CloV(x, b, fEnv) = fv
    interp(b, fEnv + (x -> av))
}
\end{verbatim}

To re-implement the interpreter in CPS, we should add the type of a
continuation.

\begin{verbatim}
type Cont = Value => Value
\end{verbatim}

A continuation takes a value of \code{Value} as input because \code{interp}
returns a value of \code{Value}. The return type of a continuation can be any
type, but like before, we choose \code{Value} just for brevity.

The following function, \code{interpCps}, is the CPS version of \code{interp}:

\begin{verbatim}
def interpCps(e: Expr, env: Env, k: Cont): Value = e match {
  ...
}
\end{verbatim}

For any \code{e}, \code{env}, and \code{k}, \code{interpCps(e, env, k)} must equal
\code{k(interp(e, env))}.

Now, we need to implement each case of the pattern matching. First, consider the
\code{Num} case. \code{interp(Num(n), env)} equals \code{NumV(n)}. Hence,
\code{k(interp(Num(n), env))} equals \code{k(NumV(n))}. Since
\code{interpCps(Num(n), env, k)} must also equal \code{k(NumV(n))}, the
\code{Num} case can be implemented as follows:

\begin{verbatim}
case Num(n) => k(NumV(n))
\end{verbatim}

It is similar to \code{k(1)} of \code{factorialCps} when \code{n} is less than
or equal to one. Since there is no need of a recursive call, the function simply
passes \code{NumV(n)} to the continuation.

The \code{Id} and \code{Fun} cases are similar to the \code{Num} case.

\begin{verbatim}
case Id(x) => k(env(x))
case Fun(x, b) => k(CloV(x, b, env))
\end{verbatim}

The remaining cases are \code{Add}, \code{Sub}, and \code{App}. They are similar
in the sense that each sort of expression consists of two subexpressions, so if you
understand one of them, the others are straightforward. Let us consider the \code{Add} case
first. The previous implementation is as follows:

\begin{verbatim}
val v1 = interp(l, env)
val v2 = interp(r, env)
add(v1, v2)
\end{verbatim}

where \code{add(v1, v2)} denotes \code{val NumV(n) = v1; val NumV(m) = v2; NumV(n
+ m)}.
Since \code{interpCps(e, env, k)} equals \code{k(interp(e, env))}, we can start
from the following code:

\begin{verbatim}
val v1 = interp(l, env)
val v2 = interp(r, env)
k(add(v1, v2))
\end{verbatim}

By desugaring variable definitions into an anonymous function and a function
application, we can find the continuation of \code{interp(l, env)}.
Recall that \code{val x = e1; e2} is equivalent to \code{(x => e2)(e1)} as shown
in \refch{first-class-functions}. Desugaring yields the following code:

\begin{verbatim}
(v1 => {
  val v2 = interp(r, env)
  k(add(v1, v2))
})(interp(l, env))
\end{verbatim}

The function applied to \code{interp(l, env)} is the continuation of
\code{interp(l, env)}. Then, \code{interp} can be replaced with
\code{interpCps} because \code{k(interp(e, env))} is the same as
\code{interpCps(e, env, k)}.

\begin{verbatim}
interpCps(l, env, v1 => {
  val v2 = interp(r, env)
  k(add(v1, v2))
})
\end{verbatim}

Now, let us focus on the body of the continuation.

\begin{verbatim}
val v2 = interp(r, env)
k(add(v1, v2))
\end{verbatim}

In a similary way, desugaring reveals the continuation of \code{interp(r, env)}.

\begin{verbatim}
(v2 =>
  k(add(v1, v2))
)(interp(r, env))
\end{verbatim}

Just like before, \code{interp} can be replaced with \code{interpCps}.

\begin{verbatim}
interpCps(r, env, v2 =>
  k(add(v1, v2))
)
\end{verbatim}

Then, we can use this new expression as the body of the continuation.

\begin{verbatim}
interpCps(l, env, v1 =>
  interpCps(r, env, v2 =>
    k(add(v1, v2))
  )
)
\end{verbatim}

Finally, we obtain the complete implementation of the \code{Add} case by
replacing \code{add} with its definition.

\begin{verbatim}
case Add(l, r) =>
  interpCps(l, env, v1 =>
    interpCps(r, env, v2 => {
      val NumV(n) = v1
      val NumV(m) = v2
      k(NumV(n + m))
    })
  )
\end{verbatim}

The code explicitly shows the continuation of each function application.
The first function application evaluates \code{l} under \code{env}. Its
continuation is \code{v1 => interpCps(r, env, v2 => k(add(v1, v2)))}.
Therefore, we can say that \code{r} is evaluated after the evaluation of
\code{l}. In \code{k(add(v1, v2))},
\code{v1} denotes the result of \code{l}. The second function application
evaluates \code{r}. Its continuation
is \code{v2 => k(add(v1, v2))}. In \code{k(add(v1, v2))}, \code{v2}
denotes the result of \code{r}. \code{k(add(v1, v2))} makes \code{NumV(n + m)}
from \code{v1} and \code{v2} and passes \code{NumV(n + m)} to \code{k}, the
continuation of evaluating \code{Add(l, r)}.

The \code{Sub} case is similar to the \code{Add} case.

\begin{verbatim}
case Sub(l, r) =>
  interpCps(l, env, v1 =>
    interpCps(r, env, v2 => {
      val NumV(n) = v1
      val NumV(m) = v2
      k(NumV(n - m))
    })
  )
\end{verbatim}

The \code{App} case is also similar but needs extra care.
The previous implementation is as follows:

\begin{verbatim}
val fv = interp(f, env)
val av = interp(a, env)
val CloV(x, b, fEnv) = fv
k(interp(b, fEnv + (x -> av)))
\end{verbatim}

By applying the same strategy, we attain the following code:

\begin{verbatim}
interpCps(f, env, fv =>
  interpCps(a, env, av => {
    val CloV(x, b, fEnv) = fv
    k(interp(b, fEnv + (x -> av)))
  })
)
\end{verbatim}

Unlike \code{Add} and \code{Sub}, an \code{interp} function call still exists.
It is not CPS because the result of the function call is used by being passed to
\code{k}. Replacing \code{interp} with \code{interpCps} resolves the problem.

\begin{verbatim}
case App(f, a) =>
  interpCps(f, env, fv =>
    interpCps(a, env, av => {
      val CloV(x, b, fEnv) = fv
      interpCps(b, fEnv + (x -> av), k)
    })
  )
\end{verbatim}

The code does not directly call \code{k}. Instead of calling \code{k}, it passes
\code{k} as an argument. Look at \code{interpCps(b, fEnv + (x -> av), k)}.
\code{k} is passed to \code{interpCps} and, therefore, eventually called.

\section{Small-Step Operational Semantics}

This section defines semantics that explicitly shows continuations for \Lang.
Previous chapters define semantics in a big-step style. Big-step semantics is
intuitive, and its inference rules give nice clues to interpreter implementers.
A single rule usually corresponds to a single case of pattern matching in
an interpreter, so it is straightforward to implement an interpreter based on
the big-step semantics and write semantics rules based on the implementation of
an interpreter.

However, it is difficult to formalize continuations in big-step semantics.
The problem is that an inference rule of big-step semantics describes the result
of an expression instead of one step of computation.
For instance, consider the following rule:

\[
  \inferrule
  { \evald{e_1}{n_1}\\
    \evald{e_2}{n_2} }
  { \evald{e_1+e_2}{n_1+n_2} }
\]

The rule implies that the value of $e_1+e_2$ is $n_1+n_2$. It does not explain
how $e_1$ evaluates to $n_1$ and $e_2$ evaluates to $n_2$. Therefore, the rule does
not describe steps that evaluate $e_1$ to $n_1$ and $e_2$ to $n_2$.
The only step the rule describes is the last step: adding $n_1$ and $n_2$.

Another problem is that rules of big-step semantics do not specify
the order of computation. Consider the above rule again. The rule does not
decide the order between $e_1$ and $e_2$. The only two things the rule requires
are that $e_1$ evaluates to $n_1$ and that $e_2$ evaluates to $n_2$.
The evaluation of $e_1$ may precede that of $e_2$, and vice versa.

These two characteristics of big-step semantics hamper us from formalizing
continuations. Continuations highly rely on precise steps of computation whose
order is fixed. Big-step semantics neither shows all the steps nor specifies the
order. Therefore, we do not use big-step semantics in this section.

\textit{Small-step operational semantics}\index{small-step operational semantics}
is another way of defining the semantics of a language. While big-step semantics
defines a relation over expressions and values, small-step semantics
defines a relation between states and states.
If one step of computation transfers a program from state $A$ to state $B$,
$A$ and $B$ are related by the relation. Such one step of computation is called
\textit{reduction}\index{reduction}. If reduction from $A$ to $B$ is possible,
we say that $A$ is reducible and reduced to $B$. On the other
hand, if a state cannot be reduced to any state, the state is irreducible.
In small-step semantics, the definition
of a state varies. One possible definition of a state is an expression.
For example, $1+2$ and $3$ are states, and $1+2$ is reduced to $3$ by one step
of computation. However, this section's small-step semantics does not use an
expression as a state. We will introduce the definition of a state soon.
The execution of a program is defined as repeated reduction. The execution
starts from an initial state, and the state becomes reduced if possible. When
no further reduction is possible, the execution stops, and the final state is
the result.

Small-step semantics fits formalizing continuations. Since execution is a
sequence of multiple reduction steps, every step of computation can be
identified. The order between the steps naturally arises. By splitting the steps
into two parts, we can formally describe a redex and a continuation.

Now, let us define states of \Lang.
A state of \Lang is a pair of a computation stack and a value stack.
The following defines computation and value stacks:

\[
\begin{array}{rrl}
k & ::= & \mtk
  \ |\  \evalk{\sigma}{e} k
  \ |\  \addk k
  \ |\  \subk k
  \ |\  \appk k \\
s & ::= & \mts
  \ |\ \conss{v} s
\end{array}
\]

where the metavariable $k$ ranges over computation stacks and the metavariable $s$
ranges over value stacks. Let $S_{\text{Comp}}$ be the set of every
computation stack and $S_{\text{Val}}$ be the set of every value stack.
We write $k\ ||\ s$ to denote a state that consists of
a computation stack $k$ and a value stack $s$. $k$ includes remaining steps
of computation, and $s$ includes values used by those steps.

A computation stack is a stack containing remaining steps of computation.
The white square, $\mtk$, denotes the empty computation stack, which
implies that there is nothing more to do.\footnote{Note that $\mtk$ has no
relation to $\square$ used to represent continuations intuitively. In the
continuation $\square+1$, $\square$ means a hole, which should be filled by
the result of a redex. On the other hand, in the small-step semantics,
$\mtk$ is the empty computation stack. The overlapped use of $\square$ is
just coincidence.} If the computation stack of a state is
$\mtk$, no further reduction is possible, and the evaluation ends.
There are four kinds of computation: $\sigma\vdash e$, $(+)$, $(-)$, and $(@)$.
Their detailed descriptions and formal definitions will be provided soon.
The computation at the top of the stack is the first step of computation. After
finishing the step, the corresponding computation in the stack is popped.
For example, $\evalk{\emptyset}{1}\mtk$ has one step of compuation:
$\emptyset\vdash1$. After finishing the step, the stack becomes $\mtk$, and the
evaluation finishes.

A value stack is a stack containing values. The black square, $\mts$, denotes the empty
value stack. Therefore, $\conss{1}\conss{2}\mts$ is a stack that contains $1$
and $2$. $1$ is at the top of the stack. Since a value stack is a stack, the
only possible operations are push and pop. If we push $0$ onto
$\conss{1}\conss{2}\mts$, we obtain $\conss{0}\conss{1}\conss{2}\mts$.
If we pop the top element from $\conss{1}\conss{2}\mts$, we obtain
$\conss{2}\mts$.

Before moving on to the formal defintion of reduction, let us see the high-level
idea of reduction first.
Each reduction step pops the element at the top of the computation stack and
manipulates the computation and value stacks depending on the popped computation.
$\sigma\vdash e$ is the only kind that pushes a value onto the value stack.
It evaluates $e$ under $\sigma$ and pushes the result of $e$ onto the value stack.
Therefore, if the current state is $\evalk{\sigma}{e}k\ ||\ s$, then the redex
is $e$, and the continuation is $k\ ||\ s$. Applying the continuation to a value is done by
pushing the value onto the value stack. Therefore, if $e$ evaluates to $v$, the
continuation is applied to $v$, so the state becomes $k\ ||\ \conss{v}s$.
For example, $\evalk{\emptyset}{1}k\ ||\ s$ is reduced to $k\ ||\ \conss{1}s$.
On the other hand, the other kinds of computation consume values in the value stack.
$(+)$ pops two values from the
value stack and then pushes the sum of the values onto the value stack.
For instance, $\addk k\ ||\ \conss{2}\conss{1}s$ is reduced to $k\ ||\ \conss{3}s$.
$(-)$ and $(@)$ are similar to $(+)$. $(-)$ performs subtraction and $(@)$
performs function application.

It becomes clear that each kind of a computation step coincides with our intuitive
notion of a computation step when we see an example. Consider the evaluation of
$1+2$ under the empty environment. The evaluation consists of three steps:
\begin{enumerate}
  \item Evaluate the expression $1$ to get the integer value $1$.
  \item Evaluate the expression $2$ to get the integer value $2$.
  \item Add the integer value $1$ to the integer value $2$ to get the integer
    value $3$.
\end{enumerate}
Steps 1 and 2 can be represented by $\emptyset\vdash1$ and $\emptyset\vdash2$,
respectively. $\emptyset\vdash1$ evaluates $1$ to get $1$ and pushes $1$ onto the
value stack for the continuation. Similarly, $\emptyset\vdash2$ evaluates $2$ to
get $2$ and pushes $2$ onto the value stack for the continuation. Finally,
step 3, which can be represented by $(+)$, pops both values and computes the sum.
The sum, $3$, is pushed onto the value stack and becomes the result of the
execution. Therefore, the evaluation of $1+2$ can be decomposed into three steps
in the computation stack:
$\evalk{\emptyset}{1} \evalk{\emptyset}{2} \addk \mtk$.

Now, we formally define reduction.
Reduction is a relation over $S_{\text{Comp}}$, $S_{\text{Val}}$,
$S_{\text{Comp}}$, and $S_{\text{Val}}$.

\[\rightarrow\subseteq S_{\text{Comp}}\times S_{\text{Val}}\times
S_{\text{Comp}} \times S_{\text{Val}}\]

We write $k_1\ ||\ s_1\rightarrow k_2\ ||\ s_2$ if $k_1\ ||\ s_1$ is reduced to
$k_2\ ||\ s_2$. For example, we write
$\evalk{\emptyset}{1}\mtk\ ||\ \mts\rightarrow \mtk\ ||\ \conss{1}\mts$

As mentioned already, execution of a program is to repeat reduction until no
more reduction is possible. When the state cannot be reduced any longer, the
execution terminates, and the state represents the result. To formalize the
notion of execution, we define the repeated reduction as the reflexive,
transitive closure of the reduction relation.

\begin{kaobox}[frametitle=Reflexive relations]
Let $A$ be a set and $R$ be a binary relation over $A$ and $A$.

If $(a,a)\in R$ for every $a\in A$, $R$ is reflexive.
\end{kaobox}

\begin{kaobox}[frametitle=Transitive relations]
Let $A$ be a set and $R$ be a binary relation over $A$ and $A$.

If $(a,b),(b,c)\in R$ implies $(a,c)\in R$ for every $a,b,c\in A$, $R$ is
transitive.
\end{kaobox}

\begin{kaobox}[frametitle={Reflexive, transitive closures}]
Let $A$ be a set and $R$ be a binary relation over $A$ and $A$.

The reflexive, transitive closure of $R$ is the smallest set $S$ such that $R\subseteq
  S\subseteq A\times A$ and $S$ is reflexive and transitive.
\end{kaobox}

$\rightarrow^\ast$ denotes repeated reduction. $k_1\ ||\ s_1\rightarrow^\ast k_n\
||\ s_n$ implies $k_1\ ||\ s_1\rightarrow k_2\ ||\ s_2$, $k_2\ ||\ s_2\rightarrow
k_3\ ||\ s_3$, …, and $k_{n-1}\ ||\ s_{n-1}\rightarrow k_n\ ||\ s_n$. The
following rules formalize the relation:

\[k\ ||\ s\rightarrow^{\ast}k\ ||\ s\]

\[
\inferrule
{ k_1\ ||\ s_1\rightarrow^{\ast}k_2 \ ||\ s_2 \\
  k_2 \ ||\ s_2 \rightarrow k_3\ ||\ s_3 }
{ k_1\ ||\ s_1\rightarrow^{\ast}k_3\ ||\ s_3 }
\]

By definition, $\rightarrow^\ast$ is indeed the reflexive, transitive
closure of $\rightarrow$. Intuitively,
$k_1\ ||\ s_1\rightarrow^{\ast}k_2\ ||\ s_2$ implies that $k_2\ ||\ s_2$
can be reached from $k_1\ ||\ s_1$ by zero or more steps of reduction.

Note that $\rightarrow^\ast$ does not require the resulting state to be
irreducible. It just denotes zero or more steps of reduction.
When $k_1\ ||\ s_1\rightarrow k_2\ ||\ s_2$ and $k_2\ ||\ s_2\rightarrow k_3\
||\ s_3$,
all of $k_1\ ||\ s_1\rightarrow^\ast k_1\
||\ s_1$, $k_1\ ||\ s_1\rightarrow^\ast k_2\ ||\ s_2$, and $k_1\ ||\
s_1\rightarrow^\ast k_3\ ||\ s_3$ are true.
Therefore, we cannot say a program that starts with $k_1\ ||\ s_1$ terminates with
$k_2\ ||\ s_2$ even if we know $k_1\ ||\ s_1\rightarrow^\ast k_2\ ||\ s_2$. $k_2\ ||\ s_2$
might not be the final state. To say that the program terminates with $k_2\ ||\
s_2$, we need an additional condition: $k_2\ ||\ s_2$ is irreducible.

In small-step
semantics, there are two kinds of termination. One kind is normal
termination, which produces a value as a result. The execution terminates
normally when the computation stack is empty. Since each reduction step pops a
computation step, reduction is impossible when the stack is empty. The empty
stack implies that there is no remaining computation, so this kind of
termination is intended. The other kind is abnormal termination, i.e.
termination due to a run-time error. It happens when the value stack contains
values that cannot be used by the current computation step. For example,
if a popped value is not an integer, both addition or subtraction are
impossible, so reduction cannot happen when the popped computation is $(+)$ or
$(-)$. It prevents further reduction even when
there is remaining computation. Therefore, such termination is considered
erroneous and harmful.

When evaluation according to the small-step semantics succesfully produces a
value, we can reach the same conclusion using the big-step semantics.
Recall that $\sigma\vdash e\Rightarrow v$ implies that $e$ evaluates to $v$
under $\sigma$. In small-step semantics, evaluation of $e$ under $\sigma$ starts
with $\evalk{\sigma}{e}\mtk\ ||\ \mts$. The redex and the continuation of the
state are $e$ and the identity function, respectively, so the state does evaluate
$e$. If the evaluation results in $v$, the final state is
$\mtk\ ||\ \conss{v}\mts$. The computation stack is empty, and the value stack
contains only $v$, which is the result. Thus, the following proposition is true.

$\forall \sigma.\forall e.\forall v.(\sigma\vdash e\Rightarrow
v)\leftrightarrow(\sigma\vdash e:: \mtk\ ||\
\mts\rightarrow^\ast\mtk\ ||\ v::\mts)$

More generally, the following statement is true:

$\forall \sigma.\forall e.\forall v.\forall k.\forall s.(\sigma\vdash
e\Rightarrow v)\leftrightarrow(\sigma\vdash e::k\ ||\
s\rightarrow^\ast k\ ||\ v::s)$

Now, we define the rules for reduction based on the interpreter implementation
of the previous section. Consider the \code{Num} case first.

\begin{verbatim}
case Num(n) => k(NumV(n))
\end{verbatim}

When $n$ is the redex and \code{k} is the continuation, the evaluation proceeds
by applying \code{k} to $n$. This state is represented by $\evalkd{n}k\
||\ s$, where the continuation \code{k} is represented by $k\ ||\ s$.
Applying $k\ ||\ s$ to $n$ is to evaluate $k\ ||\ \conss{n}s$. Therefore, we
define the following reduction rule:

\[
  \evalkd{n}k\ ||\ s\rightarrow k\ ||\ \conss{n}s
  \quad\textsc{[Red-Num]}
\]

The rule also matches our high-level intuition on reduction.
$\sigma\vdash n$ evaluates $n$ and gets $n$. After the reduction, the
computation step is removed from the computation stack, and the result is pushed
onto the value stack.

In a similar manner, we can define the rules for the \code{Id} and \code{Fun}
cases.

\begin{verbatim}
case Id(x) => k(env(x))
case Fun(x, b) => k(CloV(x, b, env))
\end{verbatim}

\[
  \evalkd{x}k\ ||\ s\rightarrow k\ ||\ \conss{\sigma(x)}s
  \quad\textsc{[Red-Id]}
\]

\[
  \evalkd{\efun{x}{e}}k\ ||\ s\rightarrow k\ ||\ \conss{\clov{x}{e}{\sigma}}s
  \quad\textsc{[Red-Fun]}
\]

Now, let us consider the \code{Add} case.

\begin{verbatim}
case Add(l, r) =>
  interpCps(l, env, v1 =>
    interpCps(r, env, v2 =>
      k(add(v1, v2))
    )
  )
\end{verbatim}

When $e_1+e_2$ is the redex, the evaluation splits into three parts:
\begin{enumerate}
  \item Evaluate $e_1$ to get $v_1$. (\code{interpCps(l, env, v1 =>})
  \item Evaluate $e_2$ to get $v_2$. (\code{interpCps(r, env, v2 =>})
  \item If both $v_1$ and $v_2$ are integers, add $v_1$ and $v_2$ to get $v_1+v_2$,
    and apply the continuation to $v_1+v_2$. (\code{k(add(v1, v2))})
\end{enumerate}
Steps 1 and 2 can be represented by $\sigma\vdash e_1$ and $\sigma\vdash e_2$,
respectively. First, $\sigma\vdash e_1$ pushes the result of $e_1$ onto the value
stack, and then $\sigma\vdash e_2$ pushes the result of $e_2$ onto the value
stack. Step 3 can be represented by $(+)$ since it pops two values from the
value stack and adds them.
From this observation, we define the following rules:

\[
  \evalkd{e_1+e_2}k\ ||\ s\rightarrow \evalkd{e_1}\evalkd{e_2}\addk k\ ||\ s
  \quad\textsc{[Red-Add1]}
\]

\[
  \addk k\ ||\ \conss{n_2}\conss{n_1}s\rightarrow k\ ||\ \conss{n_1+n_2}s
  \quad\textsc{[Red-Add2]}
\]

Rule \textsc{Red-Add1} splits the evaluation of $e_1+e_2$ into aforementioned
three parts. Then, the subsequent reduction steps evaluate $e_1$ and $e_2$ and
push their results onto the value stack. After the evaluation of $e_2$, $(+)$
is at the top of the computation stack. One more reduction step, which is
defined by Rule \textsc{Red-Add2}, pops the values
from the value stack and pushes their sum to the value stack. The following
figure summarizes this process:

\[
\begin{array}{lrcr}
& \sigma\vdash e_1+e_2::k & || & s \\
\rightarrow & \sigma\vdash e_1::\sigma\vdash e_2::\addk k & || & s \\
\rightarrow^\ast & \sigma\vdash e_2::\addk k & || & n_1::s \\
\rightarrow^\ast & \addk k & || & n_2::n_1::s \\
\rightarrow & k & || & n_1+n_2::s
\end{array}
\]

We can define the rules for the \code{Sub} case in a similar way:

\[
  \evalkd{e_1-e_2}k\ ||\ s\rightarrow \evalkd{e_1}\evalkd{e_2}\subk k\ ||\ s
  \quad\textsc{[Red-Sub1]}
\]

\[
  \subk k\ ||\ \conss{n_2}\conss{n_1}s\rightarrow k\ ||\ \conss{n_1-n_2}s
  \quad\textsc{[Red-Sub2]}
\]

The only remaining case is \code{App}.

\begin{verbatim}
case App(f, a) =>
  interpCps(f, env, fv =>
    interpCps(a, env, av => {
      val CloV(x, b, fEnv) = fv
      interpCps(b, fEnv + (x -> av), k)
    })
  )
\end{verbatim}

When $e_1\ e_2$ is the redex, the evaluation splits into three parts:
\begin{enumerate}
  \item Evaluate $e_1$ to get $v_1$. (\code{interpCps(f, env, fv =>})
  \item Evaluate $e_2$ to get $v_2$. (\code{interpCps(a, env, av =>})
  \item If $v_1$ is $\clov{x}{e}{\sigma'}$,
    evaluate $e$ under $\sigma'[x\mapsto v_2]$ with the given continuation.
    (\code{interpCps(b, fEnv + (x -> av), k)})
\end{enumerate}

The first two steps are the same as those of \code{Add} and \code{Sub}.
Therefore, we define the following rule:

\[
  \evalkd{e_1\ e_2}k\ ||\ s\rightarrow \evalkd{e_1}\evalkd{e_2}\appk k\ ||\ s
  \quad\textsc{[Red-App1]}
\]

However, the last step is a bit different. In \code{Add} and \code{Sub}, the
last step applies the continuation to a certain value, which is obtained by
addition or subtraction. In \code{App}, the body of the function must be
evaluated. Thus, we define the rule to evaluate the body with the same
continuation instead of directly applying the continuation to a particular
value.

\[
  \appk k\ ||\ \conss{v}\conss{\clov{x}{e}{\sigma}}s\rightarrow
  \evalk{\sigma\lbrack x\mapsto v\rbrack}{e}k\ ||\ s
  \quad\textsc{[Red-App2]}
\]

The following figure summarizes the evaluation of \code{App}:

\[
\begin{array}{lrcr}
  & \sigma\vdash e_1\ e_2::k & || & s \\
  \rightarrow & \sigma\vdash e_1::\sigma\vdash e_2::\appk k & || & s \\
  \rightarrow^\ast & \sigma\vdash e_2::\appk k & || & \clov{x}{e}{\sigma'}::s \\
  \rightarrow^\ast & \appk k & || & v_2::\clov{x}{e}{\sigma'}::s \\
  \rightarrow & \evalk{\sigma'[x\mapsto v_2]}{e}k & || & s \\
  \rightarrow^\ast & k & || & v::s
\end{array}
\]

The following shows the reduction steps of $(1+2)-(3+4)$ as an example:

\[
\begin{array}{lrcr}
& \emptyset\vdash(1+2)-(3+4)::\mtk &||& \mts \\
\rightarrow & \emptyset\vdash1+2::\emptyset\vdash3+4::\subk \mtk &||&
\mts \\
\rightarrow &
\emptyset\vdash1::\emptyset\vdash2::\addk \emptyset\vdash3+4::\subk \mtk &||&
\mts \\
\rightarrow & \emptyset\vdash2::\addk \emptyset\vdash3+4::\subk \mtk &||&
1::\mts \\
\rightarrow & \addk \emptyset\vdash3+4::\subk \mtk &||& 2::1::\mts \\
\rightarrow & \emptyset\vdash3+4::\subk \mtk &||& 3::\mts \\
\rightarrow & \emptyset\vdash3::\emptyset\vdash4::\addk \subk \mtk &||&
3::\mts \\
\rightarrow & \emptyset\vdash4::\addk \subk \mtk &||& 3::3::\mts \\
\rightarrow & \addk \subk \mtk &||& 4::3::3::\mts \\
\rightarrow & \subk \mtk &||& 7::3::\mts \\
\rightarrow & \mtk &||& -4::\mts \\
\end{array}
\]

The following shows the reduction steps of
$(\efun{\cx}{\efun{\cy}{\cx+\cy}})\ 1\ 2$ as an example:

\[
\begin{array}{lrcr}
              & \evalke{e\ 1\ 2} \mtk &||& \mts \\
  \rightarrow & \evalke{e\ 1} \evalke{2} \appk \mtk &||& \mts \\
  \rightarrow & \evalke{e} \evalke{1} \appk \evalke{2} \appk \mtk &||& \mts \\
  \rightarrow & \evalke{1} \appk \evalke{2} \appk \mtk &||& \conss{\langle e,\emptyset\rangle}\mts \\
  \rightarrow & \appk \evalke{2} \appk \mtk &||& \conss{1}\conss{\langle e,\emptyset\rangle}\mts \\
  \rightarrow & \evalk{\sigma_1}{\lambda \cy.\cx+\cy} \evalke{2} \appk \mtk &||& \mts \\
  \rightarrow & \evalke{2} \appk \mtk &||& \conss{\clov{\cy}{\cx+\cy}{\sigma_1}}\mts \\
  \rightarrow & \appk \mtk &||& \conss{2}\conss{\clov{\cy}{\cx+\cy}{\sigma_1}}\mts \\
  \rightarrow & \evalk{\sigma_2}{\cx+\cy} \mtk &||& \mts \\
  \rightarrow & \evalk{\sigma_2}{\cx} \evalk{\sigma_2}{\cy} \addk \mtk &||& \mts \\
  \rightarrow & \evalk{\sigma_2}{\cy} \addk \mtk &||& \conss{1}\mts \\
  \rightarrow & \addk \mtk &||& \conss{2}\conss{1}\mts \\
  \rightarrow & \mtk &||& 3::\mts \\
\end{array}
\]

where

$
\begin{array}{@{}r@{~}c@{~}l}
  e&=&\efun{\cx}{\efun{\cy}{\cx+\cy}}\\
  \sigma_1&=&[\cx\mapsto1]\\
  \sigma_2&=&[\cx\mapsto1,\cy\mapsto2]\\
\end{array}
$

\section{Exercises}

\begin{exercise}
\labex{continuations-lfae}

Complete the following \textsf{LFAE} interpreter in CPS:

\begin{verbatim}
def strict(v: Value, k: Cont): Value = v match {
  case ExprV(e, env) => ???
  case _ => k(v)
}

def interp(e: Expr, env: Env, k: Cont): Value = e match {
  case Num(n) => k(NumV(n))
  case Id(x) => k(env(x))
  case Fun(x, b) => k(CloV(x, b, env))
  case Add(l, r) => ???
  case App(f, a) => ???
}
\end{verbatim}

If $\evald{e}{v}$, then \code{interp($e$, $\sigma$, $f$)} must evaluate to $f(v)$.

\end{exercise}
