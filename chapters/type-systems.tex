\setchapterpreamble[u]{\margintoc}
\chapter{Type Systems}
\labch{type-systems}

\renewcommand{\plang}{\textsf{FAE}\xspace}
\renewcommand{\lang}{\textsf{TFAE}\xspace}

This chapter is the first chapter about typed languages. This chapter explains
the motivation of type checking and introduces a simple type
system by defining \lang, a typed variant of \plang.

\section{Run-Time Errors}

In \plang, expressions can be classified into three groups according to their
behaviors. Let us see what those three groups are. Note that in most languages,
expressions can be classified into three groups in the same manner. Thus, the
discussion of this section can be applied to various real-world languages. Just
for brevity, this section uses \plang.

The first group includes every expression that evaluates to a value. For example,
$(1+2)-3$ and $(\lambda \cx.\lambda \cy.\cx+\cy)\ 1\ 2$ belong to the first
group because $(1+2)-3$ evaluates to $0$, and $(\lambda \cx.\lambda \cy.\cx+\cy)\
1\ 2$ evaluates to $3$. Expressions in this group correspond to programs that
terminate without any problem. When we write a program, the program
usually belongs to the first group.

The second group includes every expression that never teminates. For instance,
$(\lambda \cx.\cx\ \cx)\ (\lambda \cx.\cx\ \cx)$ belongs to the second group.
The expression is function application. The first $\lambda \cx.\cx\ \cx$ is a
function, and the seceond $\lambda \cx.\cx\ \cx$ is an argument. To evaluate the
function application, the body, $\cx\ \cx$, is evaluated under the environment
that maps $\cx$ to $\lambda \cx.\cx\ \cx$. Following the content of the
environment, evaluating $\cx\ \cx$ is equivalent to evaluating $(\lambda
\cx.\cx\ \cx)\ (\lambda \cx.\cx\ \cx)$, which is the original expression. Thus,
we can say that the evaluation of $(\lambda \cx.\cx\ \cx)\ (\lambda \cx.\cx\
\cx)$ leads to the evaluation of the exactly same expression. The evaluation
runs forever and never terminates. There are many nonterminating programs in
real world. If a language supports recursive functions or loops, writing
nonterminating programs becomes much easier. Some of them are created by
programmers' mistakes. Wrong use of recursive functions or loops makes programs
run forever, contrary to the expectation of the programmers. However,
programmers sometimes intentionally write nonterminating programs.
Consider operating systems, web servers, and shells. They do not finish their
execution unless a user inputs a termination command. If an operating system
terminates although a user has not given any commands, such a behavior should
be considered as a bug. These examples clearly show the necessity of writing
nonterminating programs.

The third group includes every expression that terminates but fails to produce
a result. For example, $(\lambda \cx.\cx)+1$, $1\ 0$, and $2-\cx$
belong to the third group. The first example, $(\lambda \cx.\cx)+1$
adds a function to an integer. Since such addition is impossible, the evaluation
cannot proceed beyond the addition. Thus, the evaluation terminates at the
middle of the computation rather than reaching the final stage and producing a
result. The second example, $1\ 0$, applies an integer to a value. Functions can
be applied to values, but intgers cannot. Such an application expression also
makes the evaluation terminate abnormally. In the last example, $\cx$ is a free
identifier. Its value is unknown, so there is no way to subtract the value of
$\cx$ from $2$. Expressions in this group correspond to programs that incur
\textit{run-time errors}.\index{run-time error}

Run-time errors are always unintentional. The only reason to write expressions
that incur run-time errors is programmers' mistakes. Run-time errors terminate
programs abnormally before the programs produce meaningful results. Programmers
write programs to achieve their goals: getting particular results or performing
certain tasks forever. Run-time errors hinder programmers from achieving the
goals. Run-time errors are problematic not only to programmers but also to other
people. In commercial software, run-time errors are unpleasant experiences for users
and harm the profits and reputations of the company. Moreover,
people use programs for various purposes these days, so run-time errors can
cause much more serious problems. Programs control cars, airplanes, and medical
devices. Improper operations of such devices may kill or hurt people. A device
will operate in a weird way if the program controlling the device terminates abnormally.
Programmers surely need a way to check the existence of run-time errors before
they deploy programs.

\section{Detecting Run-Time Errors}

The simplest way to detect run-time errors is to run a program. This strategy is
called \textit{dynamic analysis}.\index{dynamic analysis} The term dynamic
means ``with execution'' (or ``during exeuction''). In general, analysis of a
program means to determine whether the program satisfies a certain property. In
this chapter, we are interested in existence of run-time errors, so analysis
means to detect run-time errors.
It is straightforward to find run-time errors in a
program with dynamic analysis. If execution finishes due to a
run-time error, the program needs revision. Otherwise, the program may
be usable without any problems.

However, dynamic analysis often fails to detect run-time errors. Execution can take
a long time or run forever. Programmers want to deploy their programs; they
cannot wait for the execution forever to finish. The dynamic analysis must stop at some point.
It makes complete prevention of run-time errors impossible. Even though a
program runs one hundred hours without any run-time errors, it can incur
a run-time error after one more hour of execution. Moreover, most
programs take inputs from users, and there are infinitely many possible inputs.
Dynamic analysis cannot cover all the cases. Even if a program runs fine for every input
being tried, the program can result in a run-time error for another input.
In addition, some programs are nondeterministic. For example, multithreaded
programs can produce different results among multiple runs because the execution of
threads is interleaved arbitrarily. Even when run-time errors are not found
during a few trials, the absence of run-time errors cannot be guaranteed.
Dynamic analysis is a simple and popular way to find run-time errors but cannot ensure
the nonexistence of run-time errors. To rule out run-time errors in programs, we
need a better way than dynamic analysis.

Since executing programs cannot prove the absence of run-time errors, we should
explore a way to detect run-time errors without executing programs. This
approach is called \textit{static analysis}.\index{static analysis} The term
static means ``without execution'' (or ``before execution''). We want to make
a program that automatically checks whether a given program can cause a run-time
error.
More precisly, we want a program $P$ that satisfies the following conditions:
\begin{itemize}
  \item For any program $I$ given as an input, $P$ outputs $\textsf{OK}$ or
    $\textsf{NOT OK}$ in a finite time.
  \item If $I$ never incurs run-time errors,
    $P(I)=\textsf{OK}$.\sidenote{$P(I)$ denotes the output of $P$ when $I$ is
    an input.}
  \item If $I$ can incur a run-time error, $P(I)=\textsf{NOT OK}$.
\end{itemize}
The first property implies that $P$ always terminates, which is different from
dynamic analysis.
The second property is called \textit{completeness},\index{completeness}
which implies that $P$ never detects run-time errors by mistake, i.e., there is
no false positive---a \textit{false positive}\index{false positive}
is to incorrectly say that there is a run-time error ($\textsf{NOT OK}$).
If $P(I)$ is $\textsf{NOT OK}$, $I$ must be able to incur a run-time
error.\sidenote{It is the contrapositive of the second property.}
The third property is called \textit{soundness},\index{soundness}
which implies that $P$ never misses run-time errors, i.e., there is no false
negative---a \textit{false negative}\index{false negative}
is to incorrectly say that there is no run-time error ($\textsf{OK}$).
If $P(I)$ is $\textsf{OK}$, $I$ must be free from run-time
errors.\sidenote{It is the contrapositive of the third property.}
$P$ can liberate programmers from the burden of detecting run-time errors.
If $P$ says $\textsf{OK}$, then the programmers do not need to worry about run-time
errors at all. If $P$ says $\textsf{NOT OK}$, then the program is certainly wrong, and
the programmers should fix the problem.

Alas, such a program $P$ does not exist. It has not existed so far and will
not exist in the future as well. In other words, the problem of deciding whether
a certain program can incur a run-time error is proven to be undecidable. The
undecidability can be proved in a similar fashion to Turing's proof of the
undecidability of the halting problem. We do not explain the proof because the proof
is outside the scope of this book.

% The proof is a proof by contradiction. Assume that $check$, which is complete and
% sound, exists. The definition of program $A$ follows:

% \[
% A(X) =
% \textsf{if}\ (\mathit{check}(X, X)=\textsf{OK})\ 1\ 1\
% \textsf{else}\ 0
% \]

% $X$ is the input for $A$. If $check(X,X)$ is $\textsf{OK}$, $A$ evaluates $1\ 1$.
% Evaluation of $1\ 1$ causes a type error since $1$ is not a function. It happens
% only if $X$ is free from type errors when $X$ itself is input. On the other hand,
% if $check(X,X)$ is $\textsf{NOT OK}$, $A$ returns $0$. It happens only if
% executing $X$ with input $X$ terminates with a type error.

% Consider $check(A,A)$. Since $check$ works for any pair $(P,I)$, $check(A,A)$
% equals either $\textsf{OK}$ or $\textsf{NOT OK}$. First, assume that the result
% is $\textsf{OK}$. It implies that $A(A)$ does not result in a type error. Because
% of the assumption, evaluation of $A(A)$ leads to evaluation of $1\ 1$, which
% causes a type error. It contradicts the assumption. Therefore, the result cannot
% be $\textsf{OK}$. Second, assume that the result is $\textsf{NOT OK}$. It implies
% that $A(A)$ results in a type error. By the assumption, $A(A)$ returns $0$
% without any type errors. It contradicts the assumption. Thus, the result cannot
% be $\textsf{NOT OK}$ either. The fact that neither $\textsf{OK}$ nor $\textsf{NOT
% OK}$ contradicts the property of $check$. The contradiction comes from assuming
% the existence of $check$. In conclusion, $check$ does not exist.

Fortunately, there is a tolerable solution. If we give up either completenss or
soundness, we can find such a program $P$. The most common choice is to forgo
completeness. Dynamic analysis is complete because a run-time error found
during execution always indicates a real bug. Dynamic analysis does not suffer
from false positives. The limitation of dynamic analysis is its unsoundness; it
can miss run-time errors. It would be better to design static analysis as a
complementary technique. $P$, which performs static analysis, should be sound at
the cost of losing completeness. Now, $P$ satisfies the following conditions:
\begin{itemize}
  \item For any given program $I$ as an input, $P$ outputs $\textsf{OK}$ or
    $\textsf{NOT OK}$ in a finite time.
  \item If $I$ never incurs run-time errors,
    both $P(I)=\textsf{OK}$ and $P(I)=\textsf{NOT OK}$ are possible.
  \item If $I$ can incur a run-time error, $P(I)=\textsf{NOT OK}$.
\end{itemize}
If $P$ says $\textsf{OK}$, it still guarantees the absence of run-time errors;
there is no false negative. However, if $P$ says $\textsf{NOT OK}$, we cannot
get any information. False positives are possible, so $P$ can say $\textsf{NOT
OK}$ even when a given program never causes a run-time error in fact.

After giving up completness, $P$ should satisfy two more conditions in order to
be practically useful. First, the number of false positives must be modest. If not,
programmers cannot get useful information from $P$. For example, we can design
$P$ to output $\textsf{NOT OK}$ in any case. Such $P$ surely satisfies the above
conditions because producing $\textsf{NOT OK}$ is allowed when a given program
never incurs run-time errors. Of course, programmers cannot get any help from
such $P$. Therefore, the number of false positives must be modest. Second, when
$P$ says $\textsf{NOT OK}$, it must provide additional information to let
programmers know why it says so. The information can help programmers decide
whether $\textsf{NOT OK}$ is a false positive or not.

Sadly, it is difficult to make $P$ that satisfies the original three conditions
and the new two conditions. It is still possible but extremely challenging.
Therefore, people forgo the detection of all the run-time errors and try to
catch a subset of them. They
classify run-time errors into two categories: type errors and the
others. \textit{Type errors}\index{type error} are run-time errors due to use of
values of wrong types. The other run-time errors are irrelevant to types.
Now, the goal of $P$ is to detect every type error. $P$ satisfies the following
conditions:
\begin{itemize}
  \item For any given program $I$ as an input, $P$ outputs $\textsf{OK}$ or
    $\textsf{NOT OK}$ in a finite time.
  \item If $I$ never incurs type errors,
    both $P(I)=\textsf{OK}$ and $P(I)=\textsf{NOT OK}$ are possible.
  \item If $I$ can incur a type error, $P(I)=\textsf{NOT OK}$.
  \item The number of false positives is modest.
  \item When $P(I)=\textsf{NOT OK}$, $P$ provides additional information about
    its decision.
\end{itemize}
Currently, there is no notion of a type. To distinguish type errors from the
others, we first need to define what a type is.

\section{Type Errors}

A \textit{type}\index{type}
is a set of values. We use types to categorize values according to their
ability and structures. Values with the same ability and structure belong to the
same type, and values with different ability or structures belong to different
types. There are various values in each programming language, and each value has its
own ability and structure. For instance, integers can be used for addition and
subtraction, while functions can be applied to values. Thus, it is natural to
classify values according to their characteristics. In \lang, the easiest way to
categorize values is to split them into numbers and functions. For example, $1$,
$42$, $0$, and $-1$ are numbers and belong to the type $\tnum$.
On the other hand, $\lambda \cx.\cx$, $\lambda \cx.\cx+\cx$, and $\lambda \cx.\cx\ 1$
are functions and belong to the type $\tfun$. Actually, this classification is
too coarse, and we will refine it later. For now, $\tnum$ and $\tfun$ are quite
enough to introduce the notion of a type.

Now, we can explain what a type error is. When a value of an unexpected type
appears, a type error happens. More precisely, if a value of the $\tfun$ type
appears where a value of the $\tnum$ type is required, or vice versa, then a
type error happens. Consider $(\lambda \cx.\cx)+1$. The first operand belongs
to $\tfun$, and the second operand belongs to $\tnum$. Addition expects both
operands to be $\tnum$. Since a value of $\tfun$ appears where a value of
$\tnum$ is expected, evaluation of the expression incurs a type error.
As another example, consider $1\ 0$. The first operand belongs to $\tnum$.
However, function application expects the first operand to be $\tfun$. A type
error happens during the evaluation because a value of $\tnum$ appears where a
value of $\tfun$ is expected.

Sometimes, it is unclear to determine whether a certain run-time error is a type error
or not. The definition of a type error can vary among people. For example,
recall the expression $2-\cx$. The expression incurs a run-time error because
$\cx$ is a free identifier. One may say this error is irrelevant to types. From
this perspective, such an error is just a free identifier error, not a type
error. However, another may say the error is relevant to types because
$\cx$, whose value is unknown, cannot belong to any type and, therefore, is not a
value of $\tnum$ although subtraction requires both operands to be $\tnum$.
From this perspective, free identifiers are just a subset of type errors. There is
no single correct answer; both perspectives make sense. This book follows the
latter, i.e., that free identifiers are type errors, because it fits the purpose
of our discussion better.

Even if we classify free identifier errors as type errors, not all run-time
errors are type errors. Some run-time errors happen even when the types of
values are correct, so they cannot be classified as type errors. We cannot find
such examples in \plang. Type errors are all of possible run-time errors in
\plang. However, in many real-world languages, which provide various features
\plang excludes, we can find run-time errors irrelevant to types. One of the
most famous examples is \code{NullPointerException} of Java. In Java,
\code{null} is a value that belongs to any type.\sidenote{In fact, Java
classifies types into primitive types and reference types, and \code{null} is a
value of any reference type.} Thus, \code{null} is a value of the \code{String}
type. Java strings provide various methods, including \code{length}, which
computes the length of the string. However, the following code incurs
\code{NullPointerException}, which is one sort of a run-time error in Java,
because computing the length of \code{null} is impossible:

\begin{verbatim}
String s = null;
s.length();
\end{verbatim}

\code{NullPointerException} is not a type error since a value of \code{String} is
expected in front of \code{.length()}, and \code{s}, which denotes \code{null},
does belong to \code{String}.

\section{Type Checking}

$P$, which detects type errors in a given program, is called a \textit{type
checker}\index{type checker}. A type error happens when a value of an unexpected
type occurs. Therefore, to find type errors, a type checker predicts the type
of the result of an expression and compares the predicted type with the expected
type.

For example, consider $e_1+e_2$. To evaluate $e_1+e_2$
without type errors, the following conditions must be satisfied:
\begin{itemize}
  \item $e_1$ does not incur a type error.
  \item $e_1$ evaluates to a value of $\tnum$ or does not terminate.
  \item $e_2$ does not incur a type error.
  \item $e_2$ evaluates to a value of $\tnum$ or does not terminate.
\end{itemize}
When the conditions are true, not only the absence of type errors in $e_1+e_2$
is guaranteed, but also we can predict the result of $e_1+e_2$:
it evaluates to a value of $\tnum$ or does not terminate.

Now, let us say that ``the type of $e$ is $\tau$'' when the following conditions
are true:
\begin{itemize}
  \item $e$ does not incur a type error.
  \item $e$ evaluates to a value of $\tau$ or does not terminate.
\end{itemize}
where the metavariable $\tau$ ranges over types. Then, we can restate the finding of
the above paragraph: when the type of $e_1$ is $\tnum$ and the type of $e_2$ is
$\tnum$, the type of $e_1+e_2$ is $\tnum$.

This example shows what a type checker does. A type checker computes the type of
an expression. When the type is successfully computed, it ensures that the
expression does not incur type errors. In this case, we say that the expression
is \textit{well-typed}\index{well-typed}.
Then, the type can be used to check
whether an expression containing the previously checked expression can cause
type errors. This process is repeated until the whole program is checked. We
call this process \textit{type checking}\index{type checking}.

A type checker requires different strategies to predict the types of different
sorts of an expression. In the above example, addition requires both
subexpressions to have $\tnum$ as their types. However, it is clear that function
application requires different types. It requires the first subexpression to
have $\tfun$ as its type because only functions can be applied to values. These
examples show that a type checker needs a separate rule for each sort of an
expression to predict the type of the expression. We call such rules
\textit{typing rules}\index{typing rule}.

There are multiple typing rules in a
single language, and we call the collection of all the typing rules in a language the
\textit{type system}\index{type system} of the language.
\textit{Static semantics}\index{static
semantics} is another name of a type system since type systems explain the
behaviors of expressions by predicting their types without execution. To
distinguish the semantics so far, which explains the behaviors of expressions by
defining their values from execution, from static semantics, we use the term
\textit{dynamic semantics}\index{dynamic semantics}.

The following table compares dynamic semantics and static semantics:

\begin{tabular}{@{~}c@{~}||@{~}c@{~}|@{~}c@{~}}
  & Dynamic semantics & Static semantics \\ \hline
  What it is for & Evaluation & Type checking \\
  Which program implements it & Interpreter & Type checker \\
  Result & Value & Type \\
\end{tabular}

Dynamic semantics defines how expressions are evaluated. By evaluation,
expressions result in values. An interpreter is a program that takes an
expression and computes its result. Static semantics defines how expressions are
type-checked. By type checking, the types of expressions are computed. A type
checker is a program that takes an expresion, predicts its type, and checks
whether run-time errors are possible. We can consider static semantics as
overapproximation of dynamic semantics. For example, dynamic semantics lets us
know that $1+2$ results in $3$, while static semantics lets us know that $1+2$
results in an integer without any run-time errors or does not terminate.

As mentioned before, the goal of a type checker, $P$, is soundness. Therefore,
the most important property of type systems is \textit{type soundness}\index{type
soundness}, or simply, just soundness. If a type checker says $\textsf{OK}$ for
a given program, then the program must never incur type errors. In this
case, we say that the program passes type checking or that the type checker accepts
the program. On the other hand, if a type checker says $\textsf{NOT OK}$ for a
given program, we cannot conclude anything, but the program might incur a type
error. In this case, we say that the type checker rejects the program.

It is nontrivial to design a sound type system for a given language. Proving the
soundness of a type system is more challenging. Proving type soundness is
beyond the scope of this book. This book introduces various type systems whose
type soundness has been proved by researchers already.

Since designing a type system and implementing a type checker are
difficult tasks, those tasks are the jobs of language designers, not language
users in most cases. Some languages come out with type systems. We call such languages
\textit{typed languages}\index{typed language} or \textit{statically typed
languages}\index{statically typed language}. The terms imply that the languages
have the notion of a type whose correct use is verified statically.
In such languages, only programs
that pass type checking can be executed. Programs rejected by the type checker
are disallowed to be exectued because their safety is not ensured. Therefore,
any execution is guaranteed to be type error free. Java, Scala,
and Rust are well-known statically typed languages in real world.

On the other hand, some languages do not provide type systems. We call such
languages \textit{untyped languages}\index{untyped language} or
\textit{dynamically typed languages}\index{dynamically typed language}.
The term untyped languages implies that they do not have type checking.
The term dynamically typed languages implies that they have the notion of a type
only at run time. Note that a type is a natural concept that exists anywhere
because values can be classified according to their characteristics in any
languages. However, in dynamically typed languages, types exist only during execution since there are no static
type checking. In such languages, programs may incur type errors during execution.
Python and JavaScript are well-known dynamically typed languages in real world.

Statically typed languages and dynamically typed languages have their own pros
and cons. Statically typed languages have the following advantages:
\begin{itemize}
  \item
Errors can be detected early. Programmers can find errors before execution.
  \item
    Static type checking
gives type information to compilers, and the compilers can optimize programs
with the information. For these reasons, programs in statically typed languages
usually outperform programs in dynamically typed languages.
  \item
    Some statically typed languages require programmers to write types explicitly
    on the source code. Such types on the code are called \textit{type
    annotations}\index{type annotation}. Type checkers verify the correctness of
    the type annotations. Thus, type annotations are automatically verified comments, which never
    become outdated, and help programmers understand the programs easily.
\end{itemize}
On the other hand, statically typed languages have the following disadvantages:
\begin{itemize}
  \item
Statically typed languages attain type soundness by giving up completeness.
Type checkers may reject programs that never incur type errors.
Therefore, programmers may waste their time in making type
checkers agree that given programs do not result in type errors.
  \item
Type annotations make code unnecessarily verbose despite their usefulness.
In addition, programmers spend their time on writing correct type annotations.
\end{itemize}

Due to these characteristics, statically typed languages are attractive when one
writes complex programs whose error detection is difficult. In addition, programs
that have to be highly trustworthy or require high performance are typical use
cases of statically typed languages. Programs that need long-term maintenance
also are good clients of statically typed languages.

The characteristics of dynamically typed languages are the opposite of
statically typed languages. Due to the lack of static type checking, errors
are discovered during execution, and programs lose some chances of optimization.
However, inconvenience due to the incompleteness of type systems disappears.
Dynamically typed languages liberate programmers from the burden of fighting
against type checking and allow them to save their time.

Therefore, dynamically-typed languages are ideal for the early stage of
development. Programmers can easily make prototypes of their programs and try
various changes in the prototypes. They do not waste their time arguing with type
checkers. Also, programs that are very small and used only a few times are where
dynamically typed languages should be used. In such applications, the advantages
of statically typed languages are worthless.

\section{\lang}

This section defines \lang, a statically typed version of \plang.

\subsection{Syntax}

The syntax of \lang is as follows:

\[
e\ ::=\ n
\ |\ e + e
\ |\ e - e
\ |\ x
\ |\ \efunt{x}{\tau}{e}
\ |\ e\ e
\]

The only difference from \plang is the type annotation of a lambda abstraction.
$\efunt{x}{\tau}{e}$ is a function whose parameter is $x$, parameter type is
$\tau$, and body is $e$. The parameter type annotation is required during
type checking, which will be explained soon.

Now, we need to define types. Classifying values into $\tnum$ and $\tfun$ like so
far is too imprecise. We need more fine-grained types for functions for a few
reasons. First, functions require arguments to belong to specific types.
Consider $\lambda\cx.\cx+\cx$. When the function is applied to a value, the
value must be a number to avoid a type error. If a function is given as an
argument, the evaluation of the body incurs a type error. Each function has its
own requirement. Therefore, the type of a function must describe the type of an
argument expected by the function. Second, different functions return different
values. Some functions returns numbers, while others return functions. To
predict the type of a function application expression, the type checker must be
able to predict the type of the return value. Thus, the type of a function must
describe the type of the return value as well.

Based on the above observations, we define types as follows:

\[
\tau\ ::=\ \tnum\ |\ \tarrow{\tau}{\tau}
\]

The type $\tnum$ is the type of every number. A type
$\tarrow{\tau_1}{\tau_2}$ is the type of a function that takes a value of
$\tau_1$ as an argument and returns a value of $\tau_2$. For example,
$\efunt{\cx}{\tnum}{\cx}$ takes a value of $\tnum$ and returns the
value. Its type is $\tarrow{\tnum}{\tnum}$.
$\efunt{\cx}{\tnum}{\efunt{\cy}{\tnum}{\cx+\cy}}$ takes a value of
$\tnum$ and returns ${\efunt{\cy}{\tnum}{\cx+\cy}}$.
${\efunt{\cy}{\tnum}{\cx+\cy}}$ also takes a value of $\tnum$. Since both $\cx$
and $\cy$ are numbers, $\cx+\cy$ also is a number, whose type is $\tnum$.
Therefore, the type of ${\efunt{\cy}{\tnum}{\cx+\cy}}$ is
$\tarrow{\tnum}{\tnum}$, and the type of
$\efunt{\cx}{\tnum}{\efunt{\cy}{\tnum}{\cx+\cy}}$ is
$\tarrow{\tnum}{(\tarrow{\tnum}{\tnum})}$. Because arrows in
function types are right associative, we can write
$\tarrow{\tnum}{\tarrow{\tnum}{\tnum}}$ instead.

A type is either $\tnum$ or $\tau_1\rightarrow\tau_2$ for some $\tau_1$
and $\tau_2$. Every value belongs to at most one type. No value is an integer and a
function at the same time. No function takes an integer as an argument and a
function as an argument at the same time. In this chapter, every value has at most
one type, and, therefore, every expression has at most one type as well. However, in some type
systems, a single value or a single expression can have multiple types.
\refch{subtype-polymorphism} shows such an example.

\subsection{Dynamic Semantics}

The dynamic semantics of \lang is similar to that of \plang. The only difference is
type annotations in lambda abstractions. Since type annotations are used only
for type checking and do not have any role at run time, they are simply ignored
when closures are constructed.

\semanticrule{Fun}{
  \evaldn{\efunt{x}{\tau}{e}}{\clov{x}{e}{\sigma}}.
}

\vspace{-1em}

\[
  \evald{\efunt{x}{\tau}{e}}{\clov{x}{e}{\sigma}}
  \quad\textsc{[Fun]}
\]

\subsection{Interpreter}

An interpreter of \lang is similar to that of \plang. Since lambda abstractions
have type annotations, the \code{Fun} case class needs a change.

\begin{verbatim}
sealed trait Expr
...
case class Fun(x: String, t: Type, b: Expr) extends Expr
\end{verbatim}

\code{Fun($x$, $\tau$, $e$)} represents $\efunt{x}{\tau}{e}$.

In addition, we define types as an ADT.

\begin{verbatim}
sealed trait Type
case object NumT extends Type
case class ArrowT(p: Type, r: Type) extends Type
\end{verbatim}

\code{NumT} represents $\tnum$, and \code{ArrowT($\tau_1$, $\tau_2$)} represents
$\tarrow{\tau_1}{\tau_2}$.

The \code{interp} function needs only one fix in the \code{Fun} case.

\begin{verbatim}
case Fun(x, _, b) => CloV(x, b, env)
\end{verbatim}

Type annotations are ignored.

\subsection{Static Semantics}

Now, we define the static semantics of \lang. One naïve approach is to define
the static semantics as a relation over expressions and types because static
semantics defines the type of each expression. However, this approach does not
work. Recall that the dynamic semantics is a
relation over environments, expressions, and values. An environment stores the
values of variables. Since variables exist both before and at run time, the static
semantics needs information about variables. While the dynamic semantics requires the
values of variables, the static semantics requires the types of variables.
To fulfill this requirement,
we introduce a type environment, which is a finite partial function from identifiers to types.
Let $T$ be the set of every type and $\embox{TEnv}$ be the set of every type
environment.

\[ \embox{TEnv} = \embox{Id}\finto\text{T} \]
\[ \Gamma \in \embox{TEnv} \]

The metavariable $\Gamma$ ranges over type environments.

The static semantics defines a relation over type environments, expressions, and types.

\[:\subseteq\embox{TEnv}\times E\times T\]

$\Gamma\vdash e:\tau$ denotes that the type of an expression $e$ under a type
environment $\Gamma$ is $\tau$. If $\emptyset\vdash e:\tau$ is true for some $\tau$,
then $e$ is well-typed, and the type system accepts the expression. If
$\emptyset\vdash e:\tau$ is false for every $\tau$, then $e$ is
\textit{ill-typed}\index{ill-typed}, i.e. not well-typed, and
the type system rejects the expression.

Let us define the typing rule for each sort of an expression.

\typerule{Typ-Num}{
  \typeofdnc{n}{\tnum}.
}

\vspace{-1em}

\[
  \typeofd{n}{\tnum}
  \quad\textsc{[Typ-Num]}
\]

The type of a number is $\tnum$.

\typerule{Typ-Add}{
  If
  \typeofdn{e_1}{\tnum} and \typeofdn{e_2}{\tnum},\\
  then
  \typeofdn{e_1+e_2}{\tnum}.
}

\vspace{-1em}

\[
  \inferrule
  { \typeofd{e_1}{\tnum} \\ \typeofd{e_2}{\tnum} }
  { \typeofd{e_1+e_2}{\tnum} }
  \quad\textsc{[Typ-Add]}
\]

If the types of $e_1$ and $e_2$ are both $\tnum$, then the type of
$e_1+e_2$ is $\tnum$.

\typerule{Typ-Sub}{
  If
  \typeofdn{e_1}{\tnum} and \typeofdn{e_2}{\tnum},\\
  then
  \typeofdn{e_1-e_2}{\tnum}.
}

\vspace{-1em}

\[
  \inferrule
  { \typeofd{e_1}{\tnum} \\ \typeofd{e_2}{\tnum} }
  { \typeofd{e_1-e_2}{\tnum} }
  \quad\textsc{[Typ-Sub]}
\]

The rule for subtraction is similar to that for addition.

\typerule{Typ-Id}{
  If $x$ is in the domain of $\Gamma$, \\
  then \typeofdn{x}{\Gamma(x)}.
}

\vspace{-1em}

\[
  \inferrule
  { x\in\dom{\Gamma} }
  { \typeofd{x}{\Gamma(x)} }
  \quad\textsc{[Typ-Id]}
\]

The dynamic semantics finds the value of a variable from an environment.
Similarly, the static semantics
finds the type of a variable from a type environment. This rule allows the
type system to detect free identifier errors.

\typerule{Typ-Fun}{
  If
  \typeofn{\Gamma[x:\tau_1]}{e}{\tau_2},\sidenote{
    Since $\mapsto$ looks similar to arrows in types, we use $:$ instead of
    $\mapsto$ to prevent confusion.} \\
  then
  \typeofdn{\efunt{x}{\tau_1}{e}}{\tarrow{\tau_1}{\tau_2}}.
}

\vspace{-1em}

\[
  \inferrule
  { \typeof{\Gamma[x:\tau_1]}{e}{\tau_2} }
  { \typeofd{\efunt{x}{\tau_1}{e}}{\tarrow{\tau_1}{\tau_2}} }
  \quad\textsc{[Typ-Fun]}
\]

The rule for a lambda abstraction needs to compute the type of a closure created
by the lambda abstraction. The type of an argument is given as $\tau_1$ by the
type annotation. The rule should determine the type of the return value of the function
as well. The return type equals the type of $e$, the function body. The value of an
argument is unknown, but the type is known as $\tau_1$. It shows why
a lambda abstraction needs a parameter type annotation. It gives information to
compute the type of the body. Since a closure captures
the environment when it is created, evaluation of its body can use variables in
the environment. Thus, computation of the type of $e$ needs every information in
$\Gamma$ and that the type of $x$ is $\tau_1$. The computation uses
$\Gamma[x:\tau_1]$. If the type of $e$ is $\tau_2$, the return type of
the function also is $\tau_2$. Finally, the type of the lambda abstraction becomes
$\tarrow{\tau_1}{\tau_2}$.

\typerule{Typ-App}{
  If
  \typeofdn{e_1}{\tarrow{\tau_1}{\tau_2}} and
  \typeofdn{e_2}{\tau_1}, \\
  then
  \typeofdn{e_1\ e_2}{\tau_2}.
}

\vspace{-1em}

\[
  \inferrule
  { \typeofd{e_1}{\tarrow{\tau_1}{\tau_2}} \\
    \typeofd{e_2}{\tau_1} }
  { \typeofd{e_1\ e_2}{\tau_2} }
  \quad\textsc{[Typ-App]}
\]

A function application expression $e_1\ e_2$ is well-typed only if $e_1$ is a function. Let
the type of $e_1$ be $\tarrow{\tau_1}{\tau_2}$. The type of the argument, $e_2$
must be $\tau_1$. The type of the return value is $\tau_2$, so the type of
$e_1\ e_2$ is $\tau_2$.

The following proof tree proves that the type of
$(\efunt{\cx}{\tnum}{\efunt{\cy}{\tnum}{\cx+\cy}})\ 1\ 2$ is $\tnum$:

\[
  \inferrule
  {
    \inferrule
    {
      \inferrule
      {
        \inferrule
        {
          \inferrule
          {
            \inferrule
            { \cx\in\dom{\Gamma_2} }
            { \typeof{\Gamma_2}{\cx}{\tnum} } \\
            \inferrule
            { \cy\in\dom{\Gamma_2} }
            { \typeof{\Gamma_2}{\cy}{\tnum} } \\
          }
          { \typeof{\Gamma_2}{\cx+\cy}{\tnum} }
        }
        { \typeof{\Gamma_1}{\efunt{\cy}{\tnum}{\cx+\cy}}{\tarrow{\tnum}{\tnum}} }
      }
      { \typeofe{e}{\tarrow{\tnum}{\tarrow{\tnum}{\tnum}}} } \\
      \typeofe{1}{\tnum}
    }
    { \typeofe{e\ 1}{\tarrow{\tnum}{\tnum}} } \\
    \typeofe{2}{\tnum}
  }
  { \typeofe{e\ 1\ 2}{
    \tnum} } \\
\]

where

$
\begin{array}{@{}r@{~}c@{~}l@{}}
  e&=&\efunt{\cx}{\tnum}{\efunt{\cy}{\tnum}{\cx+\cy}} \\
  \Gamma_1&=&[\cx:\tnum] \\
  \Gamma_2&=&[\cx:\tnum,\cy:\tnum] \\
\end{array}
$

We call a proof tree that proves the type of an expression a
\textit{type derivation}\index{type derivation}.

This type system is sound; it rejects every expression producing a
type error. For example, consider $(\efunt{\cx}{\tarrow{\tnum}{\tnum}}{\cx\ 1})\ 1$.
Evaluation of the expression results in evaluation of $1\ 1$, which causes a
type error. Since the type of $\cx\ 1$ is $\tnum$, the type of the function is
$\tarrow{(\tarrow{\tnum}{\tnum})}{\tnum}$. The function
takes an argument of type $\tarrow{\tnum}{\tnum}$. However, $1$,
the argument, has the type $\tnum$, which differs from
$\tarrow{\tnum}{\tnum}$. Therefore, the type checker rejects the expression,
which is a correct decision.

Any sound type system is incomplete. Therefore, this type system is
incomplete. The type system can reject a type-error-free expression.
Various such expressions exist. Consider
$(\efunt{\cx}{\tnum}{\cx})\ (\efunt{\cx}{\tnum}{\cx})$. The expression evaluates
to $\clov{\cx}{\cx}{\emptyset}$ without any type error. However, the type system
rejects the expression. $\efunt{\cx}{\tnum}{\cx}$
takes an argument of the type $\tnum$. However, $\efunt{\cx}{\tnum}{\cx}$,
the argument, has the type $\tarrow{\tnum}{\tnum}$,
which differs from $\tnum$. As a result, the type system rejects the
expression even though it evaluates to a value without any type error.

\subsection{Type Checker}

To implement a type checker of \lang, we first define the \code{TEnv} type,
which is the type of a type environment.

\begin{verbatim}
type TEnv = Map[String, Type]
\end{verbatim}

\code{TEnv} is a map from strings to \lang types.

The following \code{mustSame} function compares given two types:

\begin{verbatim}
def mustSame(t1: Type, t2: Type): Unit =
  if (t1 != t2)
    throw new Exception
\end{verbatim}

If the types are different, it throws an exception.

The following \code{typeCheck} function type-checks a given expression under a
given type environment.

\begin{verbatim}
def typeCheck(e: Expr, env: TEnv): Type = e match {
  case Num(n) => NumT
  case Add(l, r) =>
    mustSame(typeCheck(l, env), NumT)
    mustSame(typeCheck(r, env), NumT)
    NumT
  case Sub(l, r) =>
    mustSame(typeCheck(l, env), NumT)
    mustSame(typeCheck(r, env), NumT)
    NumT
  case Id(x) => env(x)
  case Fun(x, t, b) =>
    ArrowT(t, typeCheck(b, env + (x -> t)))
  case App(f, a) =>
    val ArrowT(t1, t2) = typeCheck(f, env)
    mustSame(t1, typeCheck(a, env))
    t2
}
\end{verbatim}

If type checking succeeds,
the function returns the type of the expression. Otherwise, it throws an exception.
Therefore, if the function throws an exception for a given expression, the
expression is ill-typed. If the function terminates without throwing an
exception, the expression is well-typed.

Each case of the pattern matching coincides with the corresponding typing rule. In the
\code{Num} case, the type is \code{NumT}. In the \code{Add} and \code{Sub} cases,
the subexpressions of the expression must have the type \code{NumT}. The type of
the expression also is \code{NumT}. The \code{Fun} case checks the type of the
function body under the extended type environment. The type of the function is
a function type. The parameter type is the same as the type annotation, and the
return type is the type of the body. The \code{App} case
checks the types of the function and the argument positions. The parameter type of the
function position must equal the type of the argument position.
The type of the application expression
is the return type of the function position.

% \section{Type Erasure}

% According to the dynamic semantics of \lang, parameter type annotations in lambda
% abstractions take no role at run time. They are necessary only for static type
% checking. Therefore, it is possible to erase type annotations in order to make
% code used at run time, while type annotations exist at compile time. Type
% erasure denotes such semantics, which removes type annotations from code used at
% run time.

% The following shows how to make an \plang counterpart of a \lang expression via type
% erasure. $\embox{erase}$ is a function from a \lang expression to an \plang
% expression.

% \[
% \begin{array}{rcl}
% \mathit{erase}(n) &=& n \\
% \mathit{erase}(e_1+e_2) &=& \mathit{erase}(e_1)+\mathit{erase}(e_2) \\
% \mathit{erase}(e_1-e_2) &=& \mathit{erase}(e_1)-\mathit{erase}(e_2) \\
% \mathit{erase}(x) &=& x \\
% \mathit{erase}(\lambda x:\tau.e) &=& \lambda x.\mathit{erase}(e) \\
% \mathit{erase}(e_1\ e_2) &=& \mathit{erase}(e_1)\ \mathit{erase}(e_2)
% \end{array}
% \]

% The only changes happen in lambda abstractions: removal of parameter type
% annotations. Integers and variables remain the same. The function is defined
% recursively for addition, subtraction, and function application.

% The following Scala code implements type erasure:

% \begin{verbatim}
% object FAE {
%   sealed trait Expr
%   case class Num(n: Int) extends Expr
%   case class Add(l: Expr, r: Expr) extends Expr
%   case class Sub(l: Expr, r: Expr) extends Expr
%   case class Id(x: String) extends Expr
%   case class Fun(x: String, b: Expr) extends Expr
%   case class App(f: Expr, a: Expr) extends Expr
% }

% def erase(e: Expr): FAE.Expr = e match {
%   case Num(n) => FAE.Num(n)
%   case Add(l, r) =>
%     FAE.Add(erase(l), erase(r))
%   case Sub(l, r) =>
%     FAE.Sub(erase(l), erase(r))
%   case Id(x) => FAE.Id(x)
%   case Fun(x, _, b) => FAE.Fun(x, erase(b))
%   case App(f, a) =>
%     FAE.App(erase(f), erase(a))
% }
% \end{verbatim}

% Since the classes represeting \lang expressions have the same names as the
% classes represeting \plang expressions, the \code{\plang} singleton object contains the
% classes for
% \plang expressions.

% \begin{verbatim}
% object FAE {
%   ...

%   sealed trait Value
%   case class NumV(n: Int) extends Value
%   case class CloV(p: String, b: FAE, e: Env) extends Value

%   type Env = Map[String, Value]

%   def interp(e: FAE, env: Env): Value = e match {
%     case Num(n) => NumV(n)
%     case Add(l, r) =>
%       val NumV(n) = interp(l, env)
%       val NumV(m) = interp(r, env)
%       NumV(n + m)
%     case Sub(l, r) =>
%       val NumV(n) = interp(l, env)
%       val NumV(m) = interp(r, env)
%       NumV(n - m)
%     case Id(x) => env(x)
%     case Fun(x, b) => CloV(x, b, env)
%     case App(f, a) =>
%       val CloV(x, b, fEnv) = interp(f, env)
%       interp(b, fEnv + (x -> interp(a, env)))
%   }
% }

% def run(e: Expr): FAE.Value = {
%   typeCheck(e, Map.empty)
%   FAE.interp(erase(e), Map.empty)
% }
% \end{verbatim}

% \code{erase} allows a \lang expression to be transformed to an \plang expression so
% that it is possible to reuse the existing \code{interp} function for \plang instead
% of
% defining a new \code{interp} function for \lang. Parameter type annotations do not
% affect the result of evaluation. The above \code{run} function is the same as the
% previous \code{run} function except that its return type is \code{\plang.Value}, not
% \code{Value}. Even though the class is different, the result represents exactly
% the
% same value. The following describes this property mathematically:

% \[
% \forall\sigma.\forall e.\forall v.
% (\sigma\vdash e\Rightarrow
% v)\leftrightarrow(\mathit{erase}(\sigma)\vdash\mathit{erase}(e)\Rightarrow\mathit{erase}(v))
% \]

% Below defines type erasure for values and environments.

% \[
% \begin{array}{rcl}
% \mathit{erase}(n) &=& n \\
% \mathit{erase}(\langle\lambda x:\tau.e,\sigma\rangle) &=& \langle\lambda
% x.\mathit{erase}(e),\mathit{erase}(\sigma)\rangle \\
% \mathit{erase}(\lbrack x_1\mapsto v_1,\cdots,x_n\mapsto v_n\rbrack) &=&\lbrack
% x_1\mapsto\mathit{erase}(v_1),\cdots,x_n\mapsto\mathit{erase}(v_n)\rbrack
% \end{array}
% \]

\section{Extending Type Systems}
\labsec{extension}

Type system designers extend type systems to enhance their usability. Type
systems can be extended in various ways. Adding new sorts of an expression or
a type is a typical way. On the other hand, it is possible to refine typing
rules without changing the syntax of a language. \refch{subtype-polymorphism}
illustrates an example of refining typing rules by adding subtyping.

There are multiple reasons to extend type systems. First, people extend type
systems to make them less incomplete. Incompleteness is an inherent limitation
of type systems. Type systems reject some expressions that do not incur type
errors. False positives cannot be eliminated completely. However, we can extend
type systems to reduce the number of false positives. By doing so, programmers
can suffer less from the misfortune that type-error-free programs are rejected.
Reducing false positives is the most common reason of extending type systems.
The subsequent four chapters show famous and practically useful type system
extensions of this kind.

Second, type systems are extended just for the convenience of programmers.
Extensions of this kind do not reduce the number of false positives. However, by
adding useful language constructs, programmers become easily able to express
their high-level ideas in source code. It is the same as extensions of
dynamically typed languages
described by previous chapters. For example, it is possible to define recursive
functions in \plang. However, defining recursive functions in \plang is difficult
and complex. To resolve the problem, we define \textsf{RFAE} by extending \plang
with primitive support for recursive functions. Recursive functions can be
defined much easily in \textsf{RFAE} than \plang. Statically typed languages can
be improved in similar ways.

Third, more run-time errors can be considered as type errors by extending type
systems. In most real-world languages, some run-time errors are not type errors.
For example, recall \code{NullPointerException} of Java. Since Java does not
classify \code{NullPointerException} as a type error, the Java type checker does
not detect \code{NullPointerException}, and every Java program suffers from the
possibility of \code{NullPointerException}. \code{NullPointerException} is one
of the most commonly occuring run-time errors in Java. Kotlin introduces the
notion of an explicit null type by extending the type system of Java. In Kotlin,
\code{null} is not a value of \code{String}, and \code{NullPointerException} is
considered as a type error. If a Kotlin program passes type checking, it is free
of \code{NullPointerException}. Extending type systems to detect more run-time
errors is valuable but can increase false positives. Thus, type system designers
always consider the tradeoff between enlarging the set of type errors and reducing false
positives.

Now, let us consider two simple extensions of \lang: local variable definitions
and pairs.

\subsection{Local Variable Definitions}

Local variable definitions (\textsf{val} expressions)
are the second kind of an extension. Even without
local variable definitions, programmers can write the equivalent code with
functions. However, local variable definitions help them write
code concisely.

The syntax and dynamic semantics of local variable definitions follow \textsf{VAE}.
The static semantics is as follows:

\typerule{Typ-Val}{
  If
    \typeofdn{e_1}{\tau_1} and
    \typeofn{\Gamma[x:\tau_1]}{e_2}{\tau_2}, \\
  then
    \typeofdn{\ebind{x}{e_1}{e_2}}{\tau_2}.
}

\vspace{-1em}

\[
  \inferrule
  {
    \typeofd{e_1}{\tau_1} \\
    \typeof{\Gamma[x:\tau_1]}{e_2}{\tau_2}
  }
  { \typeofd{\ebind{x}{e_1}{e_2}}{\tau_2} }
  \quad\textsc{[Typ-Val]}
\]

Note that local variable definitions do not require type annotations, while
lambda abstractions do. Therefore, local variable definitions are more
convenient then lambda abstractions for binding.

\subsection{Pairs}

Pairs are the first kind of an extension. In \plang, we can desugar pairs to
functions:
\begin{itemize}
  \item $(e_1,e_2)$, which creates a new pair, is desugared to $\efun{\cf}{\cf\ e_1\
    e_2}$.\sidenote{Strictly speaking, the correct desugaring is
    $(\efun{\cx}{\efun{\cy}{\efun{\cf}{\cf\ \cx\ \cy}}})\ e_1\ e_2$ in eager
    languages like \plang, but we use the simpler one here.}
  \item $e\textsf{.1}$, which acquires the first element of a pair,
    is desugared to $e\ \efun{\cx}{\efun{\cy}{\cx}}$.
  \item $e\textsf{.2}$, which acquires the second element of a pair,
    is desugared to $e\ \efun{\cx}{\efun{\cy}{\cy}}$.
\end{itemize}
However, such expressions are ill-typed in \lang.
When the type of $e_1$ is $\tnum$ and the type of $e_2$ is $\tarrow{\tnum}{\tnum}$,
$\efun{\cf}{\cf\ e_1\ e_2}$ is a function that returns $\tnum$ in some cases and
$\tarrow{\tnum}{\tnum}$ in some other cases. There is no way to represent the
type of such a function. Thus, programs using pairs cannot be written in \lang.

To overcome the limitation, we extend \lang to support pairs.
The syntax and dynamic semantics of pairs follow
Exercise~7 of \refch{first-class-functions}.
We add pair types as follows:

\[ \tau \ ::= \ \cdots\ | \ \tau\times\tau \]

A type $\tau_1\times\tau_2$ is the type of $(v_1,v_2)$ if the type of $v_1$ is
$\tau_1$ and the type of $v_2$ is $\tau_2$.
The following rules define the static semantics:

\typerule{Typ-Pair}{
  If
    \typeofdn{e_1}{\tau_1} and
    \typeofdn{e_2}{\tau_2}, \\
  then
    \typeofdn{(e_1,e_2)}{\tau_1\times\tau_2}.
}

\vspace{-1em}

\[
  \inferrule
  { \typeofd{e_1}{\tau_1} \\
    \typeofd{e_2}{\tau_2} }
  { \typeofd{(e_1,e_2)}{\tau_1\times\tau_2} }
  \quad\textsc{[Typ-Pair]}
\]

\typerule{Typ-Fst}{
  If \typeofdn{e}{\tau_1\times\tau_2}, \\
  then \typeofdn{e\textsf{.1}}{\tau_1}.
}

\vspace{-1em}

\[
  \inferrule
  { \typeofd{e}{\tau_1\times\tau_2} }
  { \typeofd{e\textsf{.1}}{\tau_1} }
  \quad\textsc{[Typ-Fst]}
\]

\typerule{Typ-Snd}{
  If \typeofdn{e}{\tau_1\times\tau_2}, \\
  then \typeofdn{e\textsf{.2}}{\tau_2}.
}

\vspace{-1em}

\[
  \inferrule
  { \typeofd{e}{\tau_1\times\tau_2} }
  { \typeofd{e\textsf{.2}}{\tau_2} }
  \quad\textsc{[Typ-Snd]}
\]

\section{Exercises}

\begin{enumerate}
\item This exercise extends \lang with lists.
  \[
    \begin{array}{r@{~~}r@{~~}l}
      e & ::= & \cdots
      \ |\ \textsf{nil}[\tau]
      \ |\ \textsf{cons}\ e\ e
      \ |\ \textsf{head}\ e
      \ |\ \textsf{tail}\ e \\
      \tau & ::= & \cdots\ |\ \textsf{list}\ \tau \\
    \end{array}
  \]
Write the typing rules of the added expressions.

\item This exercise extends \lang with boxes.
  \[
    \begin{array}{r@{~~}r@{~~}l}
      e & ::= & \cdots
      \ |\ \eref{e}
      \ |\ \ederef{e}
      \ |\ \eset{e}{e}
      \ |\ \eseq{e}{e} \\
      \tau & ::= & \cdots\ |\ \textsf{box}\ \tau \\
    \end{array}
  \]
      The dynamic semantics of boxes is the same as \textsf{BFAE}.
\begin{enumerate}
  \item Write the typing rules of the added expressions.
  Assignments should not change the types of the values at given locations.
  \item Draw the type derivation of the following expression:\\
  $
  \begin{array}{l}
    \ebind{\cx}{\eref{3}}{\\
    \ebind{\cy}{\ederef{\cx}+7}{\\
    \eseq{\eset{\cx}{8}}{\\
    \cy+\ederef{\cx}
    }}}
  \end{array}
  $
\end{enumerate}

\item This exercise extends \lang with mutable variables.
  \[ e\ ::=\ \cdots\ |\ \eset{x}{e} \]
    The dynamic semantic of mutable variables is the same as \textsf{MFAE}.
\begin{enumerate}
  \item Write the typing rule of the added expression.
\end{enumerate}
Now, we extend the language again with pointers.
\[
  \begin{array}{r@{~~}r@{~~}l}
    e & ::= & \cdots\ |\ \ast e \ |\ \& x\ |\ \ast\eset{e}{e} \\
    \tau & ::= & \cdots\ |\ \tau\ast
  \end{array}
\]
The dynamic semantic of pointers is the same as Exercise~1 of
\refch{mutable-variables}.
A type $\tau\ast$ denotes the address type of a type $\tau$.
For example, for a given address $a$, if the value at $a$ is a number,
then the type of $a$ is $\tnum\ast$.

\begin{enumerate}
  \setcounter{enumii}{1}
  \item Write the typing rules of the added expressions.
\end{enumerate}

% \item Consider the following language:
% \[
% \begin{array}{rllrll}
% p ::= & m^*\ e &
% \multicolumn{4}{l}{\mbox{program consists of 0 or more function declarations followed by an expression}}\\
% m ::= & \verb!fun !f \verb!(!x:\tau\verb!) = !\ e & \mbox{function declaration}\\
% e ::= & n&\mbox{number}
% &v ::= & n&\mbox{number}\\
% \mid&b&\mbox{boolean}
% &\mid&b&\mbox{boolean}\\
% \mid&e\ \verb!+!\ e&\mbox{addition}
% &\tau ::= &\verb+num+ &\mbox{type of numbers}\\
% \mid&\verb!not!\ e&\mbox{negation}
% &\mid&\verb+bool+&\mbox{type of booleans}\\
% \mid&x&\mbox{identifier}\\
% \mid&f\ e&\mbox{function application}\\
% x \in&\embox{Id} && f \in& \embox{FunctionName}\\
% n \in&\embox{Number}&& b \in& \embox{Boolean}\\
% v \in&\embox{Val} && \sigma \in& \embox{Id} \rightarrow \embox{Val}
% \end{array}
% \]
% The language supports multiple function declarations with the same name.
% The static and dynamic semantics of the language are as follows:
% \[
% \begin{array}{ccccc}
% \emptyset \vdash n : \texttt{num}
% &&
% \emptyset \vdash b : \texttt{bool}
% \\[2em]
% \newinfrule
% {\mbox{\emph{noDuplicate}}(m^*)\qquad m^*, \emptyset \vdash e \Rightarrow v}
% {\emptyset\vdash m^*\ e \Rightarrow v}
% &\qquad &
% m^*, \sigma\vdash n \Rightarrow n
% &\qquad &
% m^*, \sigma\vdash b \Rightarrow b
% \\[2em]

% \newinfrule
% {m^*, \sigma\vdash e_1 \Rightarrow v_1\qquad m^*, \sigma\vdash e_2 \Rightarrow v_2}
% {m^*, \sigma\vdash e_1\ \texttt{+}\ e_2 \Rightarrow v_1 + v_2}
% &\qquad &
% \newinfrule
% {m^*, \sigma\vdash e \Rightarrow b}
% {m^*, \sigma\vdash \texttt{not}\ e \Rightarrow \neg b}
% &\qquad &
% \newinfrule
% {x \in \mbox{\emph{Dom}}(\sigma)}
% {m^*, \sigma\vdash x \Rightarrow \sigma(x)}
% \\[3em]
% \lefteqn{
% \newinfrule
% {\begin{array}{c}
% \mbox{\emph{findDecl}}(m^*, f) = \{
% \texttt{fun}\ f \texttt{(}x_1:\tau_1 \texttt{) =}\ e_1,\ \cdots,\
% \texttt{fun}\ f \texttt{(}x_n:\tau_n \texttt{) =}\ e_n \}\\
% m^*, \sigma \vdash e \Rightarrow v\qquad
% \emptyset \vdash v : \tau\qquad
% \tau = \tau_i\qquad
% m^*, [x_i \mapsto v] \vdash e_i \Rightarrow v'
% \end{array}
% }
% {m^*, \sigma\vdash f\ e \Rightarrow v'}}
% \end{array}
% \]
% where
% $\mbox{\emph{noDuplicate}}(m^*)$ checks that any two function declarations in $m^*$
% have either different names or different parameter types, and
% $\mbox{\emph{findDecl}}(m^*, f)$ returns a set of all the function declarations among $m^*$ of the name $f$.

% Draw the semantics derivation of the following program:\\
% {
% \begin{verbatim}
% fun f(x: num) = g x
% fun g(y: bool) = not y
% fun g(z: num) = z + 42
% f 8
% \end{verbatim}
% }

\end{enumerate}
